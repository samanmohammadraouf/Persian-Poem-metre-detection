{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c3tZ7UAh7Y_r",
    "outputId": "49c5dc1e-920a-41e2-b27d-7e92c7929049"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.10/dist-packages (1.5.1)\n",
      "Requirement already satisfied: numpy<2.0,>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.26.4)\n",
      "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (24.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.5.0+cu121)\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (0.11.8)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.1.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.12.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.16.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2024.6.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.10.0->torchmetrics) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->torchmetrics) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Cfyl-MlZYJv"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import MT5ForConditionalGeneration, MT5Tokenizer\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import nltk\n",
    "import random\n",
    "import torchmetrics\n",
    "from torchmetrics.text import BLEUScore, ROUGEScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6cmPGQs-Z6Lc",
    "outputId": "be00145c-19e7-4e98-c539-ec7ae9335707"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wv1TN_vhZ7Bk"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a1x24UW-Z7tv",
    "outputId": "339e6d46-43d8-472d-e2dc-2989c3410bb0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SG2eGfaqZ9GX",
    "outputId": "96d28a7b-f20f-4a2b-e778-81c324e1a302"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "mt5_tokenizer = MT5Tokenizer.from_pretrained(\"google/mt5-small\")\n",
    "mt5_model = MT5ForConditionalGeneration.from_pretrained(\"google/mt5-small\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D1CY3CXmZ-Hi"
   },
   "outputs": [],
   "source": [
    "class SimpleSpaceTokenizer:\n",
    "    def __init__(self):\n",
    "        self.token2id = {}\n",
    "        self.id2token = {}\n",
    "        self.vocab_size = 0\n",
    "        self.eos_token_id = None\n",
    "\n",
    "    def fit_on_texts(self, texts):\n",
    "        unique_tokens = set()\n",
    "        for text in texts:\n",
    "            tokens = text.split(\" \")\n",
    "            unique_tokens.update(tokens)\n",
    "\n",
    "        self.token2id = {token: idx for idx, token in enumerate(unique_tokens, start=1)}\n",
    "        self.eos_token_id = len(self.token2id) + 1  # Assign a unique ID to the EOS token\n",
    "        self.token2id[\"<EOS>\"] = self.eos_token_id\n",
    "\n",
    "        self.id2token = {idx: token for token, idx in self.token2id.items()}\n",
    "        self.vocab_size = len(self.token2id) + 1  # Adding 1 for padding token\n",
    "\n",
    "    def tokenize(self, texts, max_length=48):\n",
    "        tokenized_texts = []\n",
    "        for text in texts:\n",
    "            tokens = text.split(\" \")\n",
    "            token_ids = [self.token2id.get(token, 0) for token in tokens]\n",
    "            token_ids = token_ids[:max_length - 1]  # Reserve space for EOS token\n",
    "            token_ids.append(self.eos_token_id)  # Add EOS token\n",
    "\n",
    "            padding_length = max_length - len(token_ids)\n",
    "            token_ids += [0] * padding_length  # Add padding tokens\n",
    "            tokenized_texts.append(token_ids)\n",
    "        return torch.tensor(tokenized_texts)\n",
    "\n",
    "    def decode(self, token_ids):\n",
    "        return \" \".join([self.id2token.get(token_id, \"\") for token_id in token_ids if token_id != 0 and token_id != self.eos_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M66KFR4KZ_Br"
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('/content/drive/MyDrive/LanguageUnderstanding/HW1/train_samples.csv')\n",
    "poem_text = \"وزن مصرع داده شده  \" + train_data['poem_text'] + \" برابر است با \"\n",
    "\n",
    "# Tokenize the inputs\n",
    "inputs = mt5_tokenizer(poem_text.tolist(), padding=True, truncation=True, return_tensors=\"pt\", max_length=18)\n",
    "input_ids = inputs['input_ids'].squeeze().to(device)\n",
    "attention_mask = inputs['attention_mask'].squeeze().to(device)\n",
    "\n",
    "# Process labels\n",
    "metre = train_data['metre'].astype(str)\n",
    "label_tokenizer = SimpleSpaceTokenizer()\n",
    "label_tokenizer.fit_on_texts(metre.tolist())\n",
    "labels = label_tokenizer.tokenize(metre.tolist(), max_length=7).to(device)\n",
    "\n",
    "# Create DataLoader for training data\n",
    "train_loader = DataLoader(torch.utils.data.TensorDataset(input_ids, attention_mask, labels), batch_size=320, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VyZUb9mi48MN",
    "outputId": "15670c35-1745-4bb6-c9aa-182824c3cc07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-de18h4WZ_0n"
   },
   "outputs": [],
   "source": [
    "val_data = pd.read_csv(f'/content/drive/MyDrive/LanguageUnderstanding/HW1/validation_samples.csv')\n",
    "\n",
    "val_poem_text = \"وزن مصرع داده شده  \" + val_data['poem_text'] + \" برابر است با \"\n",
    "val_metre = val_data['metre'].astype(str)\n",
    "\n",
    "val_inputs = mt5_tokenizer(val_poem_text.tolist(), padding=True, truncation=True, return_tensors=\"pt\", max_length=18)\n",
    "val_input_ids = val_inputs['input_ids'].squeeze().to(device)\n",
    "val_attention_mask = val_inputs['attention_mask'].squeeze().to(device)\n",
    "val_labels = label_tokenizer.tokenize(val_metre.tolist(), max_length=6).to(device)\n",
    "\n",
    "val_loader = DataLoader(torch.utils.data.TensorDataset(val_input_ids, val_attention_mask, val_labels), batch_size=320, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nekpvmNgaAuV"
   },
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(f'/content/drive/MyDrive/LanguageUnderstanding/HW1/test_samples.csv')\n",
    "\n",
    "test_poem_text = \"وزن مصرع داده شده را پیدا کن \" + test_data['poem_text'] + \" برابر است با \"\n",
    "\n",
    "test_inputs = mt5_tokenizer(test_poem_text.tolist(), padding=True, truncation=True, return_tensors=\"pt\", max_length=18)\n",
    "\n",
    "test_input_ids = test_inputs['input_ids'].squeeze().to(device)\n",
    "test_attention_mask = test_inputs['attention_mask'].squeeze().to(device)\n",
    "\n",
    "test_loader = DataLoader(torch.utils.data.TensorDataset(test_input_ids, test_attention_mask), batch_size=320, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aa7-lnHcbGOj"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Feq4196FcYyR"
   },
   "outputs": [],
   "source": [
    "def train_mt5(model, train_loader, val_loader, epochs=3, lr=1e-3):\n",
    "    model.train()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.90)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    teacher_forcing_ratio = 1.0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "            # Start token\n",
    "            decoder_input_ids = torch.full(\n",
    "                (input_ids.size(0), 1), model.config.decoder_start_token_id, dtype=torch.long, device=device\n",
    "            )\n",
    "\n",
    "            loss = torch.tensor(0.0, device=device)\n",
    "            for t in range(labels.size(1) - 1):  \n",
    "                # Forward pass\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, decoder_input_ids=decoder_input_ids)\n",
    "                logits = outputs.logits[:, -1, :]  # Get logits for the last generated token\n",
    "\n",
    "                target_token = labels[:, t].view(-1)  # Ensure correct shape\n",
    "                loss += loss_fn(logits, target_token)\n",
    "\n",
    "                use_teacher_forcing = random.random() < teacher_forcing_ratio\n",
    "\n",
    "                if use_teacher_forcing:\n",
    "                    next_token = target_token.unsqueeze(1) \n",
    "                else:\n",
    "                    next_token = torch.argmax(logits, dim=-1).unsqueeze(1) \n",
    "\n",
    "                decoder_input_ids = torch.cat([decoder_input_ids, next_token], dim=1)\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() / (labels.size(1) - 1)  # Average loss per token\n",
    "            print(f'batch {i}/ {len(train_loader)}, loss training: {loss:.4f}')\n",
    "\n",
    "        # Decay teacher forcing ratio\n",
    "        teacher_forcing_ratio = max(0.5, teacher_forcing_ratio * 0.9)  # Gradually reduce but not less than 0.5\n",
    "\n",
    "        scheduler.step()\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = torch.tensor(0.0, device=device)\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids, attention_mask, labels = batch\n",
    "                input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "\n",
    "                # Start token\n",
    "                decoder_input_ids = torch.full(\n",
    "                    (input_ids.size(0), 1), model.config.decoder_start_token_id, dtype=torch.long, device=device\n",
    "                )\n",
    "\n",
    "                loss = torch.tensor(0.0, device=device)\n",
    "                for t in range(labels.size(1) - 1):\n",
    "                    outputs = model(input_ids=input_ids, attention_mask=attention_mask, decoder_input_ids=decoder_input_ids)\n",
    "                    logits = outputs.logits[:, -1, :]  \n",
    "\n",
    "                    target_token = labels[:, t]\n",
    "                    loss += loss_fn(logits, target_token)\n",
    "\n",
    "                    next_token = target_token.unsqueeze(1)\n",
    "                    decoder_input_ids = torch.cat([decoder_input_ids, next_token], dim=1)\n",
    "\n",
    "                val_loss += loss.item() / (labels.size(1) - 1)\n",
    "        avg_val_loss = val_loss.item() / len(val_loader)\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Validation Loss: {avg_val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D5TE7IILcaP9",
    "outputId": "6299ed52-1a63-48d0-c3f3-f572f4fb1d66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0/ 2342, loss training: 2.3105\n",
      "batch 1/ 2342, loss training: 2.3746\n",
      "batch 2/ 2342, loss training: 2.5680\n",
      "batch 3/ 2342, loss training: 2.2195\n",
      "batch 4/ 2342, loss training: 2.5250\n",
      "batch 5/ 2342, loss training: 2.3813\n",
      "batch 6/ 2342, loss training: 2.2816\n",
      "batch 7/ 2342, loss training: 2.3634\n",
      "batch 8/ 2342, loss training: 2.4087\n",
      "batch 9/ 2342, loss training: 2.3054\n",
      "batch 10/ 2342, loss training: 2.4575\n",
      "batch 11/ 2342, loss training: 2.2779\n",
      "batch 12/ 2342, loss training: 2.3027\n",
      "batch 13/ 2342, loss training: 2.4027\n",
      "batch 14/ 2342, loss training: 2.1775\n",
      "batch 15/ 2342, loss training: 2.3618\n",
      "batch 16/ 2342, loss training: 2.2191\n",
      "batch 17/ 2342, loss training: 2.2031\n",
      "batch 18/ 2342, loss training: 2.2760\n",
      "batch 19/ 2342, loss training: 2.3355\n",
      "batch 20/ 2342, loss training: 2.4008\n",
      "batch 21/ 2342, loss training: 2.5288\n",
      "batch 22/ 2342, loss training: 2.3350\n",
      "batch 23/ 2342, loss training: 2.3693\n",
      "batch 24/ 2342, loss training: 2.2473\n",
      "batch 25/ 2342, loss training: 2.3482\n",
      "batch 26/ 2342, loss training: 2.3805\n",
      "batch 27/ 2342, loss training: 2.3844\n",
      "batch 28/ 2342, loss training: 2.2473\n",
      "batch 29/ 2342, loss training: 2.2837\n",
      "batch 30/ 2342, loss training: 2.3524\n",
      "batch 31/ 2342, loss training: 2.2875\n",
      "batch 32/ 2342, loss training: 2.2251\n",
      "batch 33/ 2342, loss training: 2.1103\n",
      "batch 34/ 2342, loss training: 2.1984\n",
      "batch 35/ 2342, loss training: 2.2732\n",
      "batch 36/ 2342, loss training: 2.1956\n",
      "batch 37/ 2342, loss training: 2.2637\n",
      "batch 38/ 2342, loss training: 2.2723\n",
      "batch 39/ 2342, loss training: 2.2320\n",
      "batch 40/ 2342, loss training: 2.0880\n",
      "batch 41/ 2342, loss training: 2.2955\n",
      "batch 42/ 2342, loss training: 2.0225\n",
      "batch 43/ 2342, loss training: 2.3127\n",
      "batch 44/ 2342, loss training: 2.0588\n",
      "batch 45/ 2342, loss training: 2.1131\n",
      "batch 46/ 2342, loss training: 2.2315\n",
      "batch 47/ 2342, loss training: 2.2510\n",
      "batch 48/ 2342, loss training: 2.3328\n",
      "batch 49/ 2342, loss training: 2.2732\n",
      "batch 50/ 2342, loss training: 2.1567\n",
      "batch 51/ 2342, loss training: 2.1272\n",
      "batch 52/ 2342, loss training: 2.2196\n",
      "batch 53/ 2342, loss training: 2.0961\n",
      "batch 54/ 2342, loss training: 2.2105\n",
      "batch 55/ 2342, loss training: 2.1361\n",
      "batch 56/ 2342, loss training: 2.2146\n",
      "batch 57/ 2342, loss training: 2.2512\n",
      "batch 58/ 2342, loss training: 2.0545\n",
      "batch 59/ 2342, loss training: 2.1768\n",
      "batch 60/ 2342, loss training: 2.2130\n",
      "batch 61/ 2342, loss training: 2.1599\n",
      "batch 62/ 2342, loss training: 2.0542\n",
      "batch 63/ 2342, loss training: 2.1636\n",
      "batch 64/ 2342, loss training: 2.1594\n",
      "batch 65/ 2342, loss training: 2.0867\n",
      "batch 66/ 2342, loss training: 2.1329\n",
      "batch 67/ 2342, loss training: 2.0734\n",
      "batch 68/ 2342, loss training: 2.1541\n",
      "batch 69/ 2342, loss training: 2.1583\n",
      "batch 70/ 2342, loss training: 1.9541\n",
      "batch 71/ 2342, loss training: 2.1278\n",
      "batch 72/ 2342, loss training: 2.1233\n",
      "batch 73/ 2342, loss training: 2.1879\n",
      "batch 74/ 2342, loss training: 2.0874\n",
      "batch 75/ 2342, loss training: 2.2601\n",
      "batch 76/ 2342, loss training: 2.1347\n",
      "batch 77/ 2342, loss training: 2.0865\n",
      "batch 78/ 2342, loss training: 2.0543\n",
      "batch 79/ 2342, loss training: 2.1283\n",
      "batch 80/ 2342, loss training: 2.0268\n",
      "batch 81/ 2342, loss training: 2.1368\n",
      "batch 82/ 2342, loss training: 2.0017\n",
      "batch 83/ 2342, loss training: 2.2554\n",
      "batch 84/ 2342, loss training: 2.0497\n",
      "batch 85/ 2342, loss training: 2.0605\n",
      "batch 86/ 2342, loss training: 2.0863\n",
      "batch 87/ 2342, loss training: 2.1286\n",
      "batch 88/ 2342, loss training: 2.0843\n",
      "batch 89/ 2342, loss training: 1.9500\n",
      "batch 90/ 2342, loss training: 2.2248\n",
      "batch 91/ 2342, loss training: 2.1133\n",
      "batch 92/ 2342, loss training: 2.1114\n",
      "batch 93/ 2342, loss training: 2.0796\n",
      "batch 94/ 2342, loss training: 2.1705\n",
      "batch 95/ 2342, loss training: 1.9145\n",
      "batch 96/ 2342, loss training: 2.0189\n",
      "batch 97/ 2342, loss training: 2.0450\n",
      "batch 98/ 2342, loss training: 2.0653\n",
      "batch 99/ 2342, loss training: 2.0572\n",
      "batch 100/ 2342, loss training: 2.1854\n",
      "batch 101/ 2342, loss training: 1.9616\n",
      "batch 102/ 2342, loss training: 2.0381\n",
      "batch 103/ 2342, loss training: 1.9630\n",
      "batch 104/ 2342, loss training: 2.0553\n",
      "batch 105/ 2342, loss training: 2.0662\n",
      "batch 106/ 2342, loss training: 2.1143\n",
      "batch 107/ 2342, loss training: 2.1557\n",
      "batch 108/ 2342, loss training: 1.9999\n",
      "batch 109/ 2342, loss training: 2.1228\n",
      "batch 110/ 2342, loss training: 2.1457\n",
      "batch 111/ 2342, loss training: 2.0221\n",
      "batch 112/ 2342, loss training: 1.9330\n",
      "batch 113/ 2342, loss training: 1.9320\n",
      "batch 114/ 2342, loss training: 2.0966\n",
      "batch 115/ 2342, loss training: 1.8720\n",
      "batch 116/ 2342, loss training: 2.0172\n",
      "batch 117/ 2342, loss training: 1.8442\n",
      "batch 118/ 2342, loss training: 2.0464\n",
      "batch 119/ 2342, loss training: 2.2189\n",
      "batch 120/ 2342, loss training: 1.8948\n",
      "batch 121/ 2342, loss training: 2.1458\n",
      "batch 122/ 2342, loss training: 2.1586\n",
      "batch 123/ 2342, loss training: 1.9360\n",
      "batch 124/ 2342, loss training: 1.9000\n",
      "batch 125/ 2342, loss training: 2.0510\n",
      "batch 126/ 2342, loss training: 2.0644\n",
      "batch 127/ 2342, loss training: 2.0882\n",
      "batch 128/ 2342, loss training: 2.1710\n",
      "batch 129/ 2342, loss training: 2.0292\n",
      "batch 130/ 2342, loss training: 2.1775\n",
      "batch 131/ 2342, loss training: 2.0550\n",
      "batch 132/ 2342, loss training: 1.7997\n",
      "batch 133/ 2342, loss training: 1.9513\n",
      "batch 134/ 2342, loss training: 2.0719\n",
      "batch 135/ 2342, loss training: 2.0628\n",
      "batch 136/ 2342, loss training: 2.0945\n",
      "batch 137/ 2342, loss training: 2.0518\n",
      "batch 138/ 2342, loss training: 1.8538\n",
      "batch 139/ 2342, loss training: 2.0196\n",
      "batch 140/ 2342, loss training: 1.8363\n",
      "batch 141/ 2342, loss training: 1.9072\n",
      "batch 142/ 2342, loss training: 1.8555\n",
      "batch 143/ 2342, loss training: 1.9434\n",
      "batch 144/ 2342, loss training: 1.8770\n",
      "batch 145/ 2342, loss training: 1.8688\n",
      "batch 146/ 2342, loss training: 1.9087\n",
      "batch 147/ 2342, loss training: 1.8992\n",
      "batch 148/ 2342, loss training: 1.9175\n",
      "batch 149/ 2342, loss training: 1.7228\n",
      "batch 150/ 2342, loss training: 1.8503\n",
      "batch 151/ 2342, loss training: 1.9335\n",
      "batch 152/ 2342, loss training: 1.8887\n",
      "batch 153/ 2342, loss training: 1.8858\n",
      "batch 154/ 2342, loss training: 1.6859\n",
      "batch 155/ 2342, loss training: 1.9445\n",
      "batch 156/ 2342, loss training: 1.8508\n",
      "batch 157/ 2342, loss training: 1.8021\n",
      "batch 158/ 2342, loss training: 1.9043\n",
      "batch 159/ 2342, loss training: 1.9091\n",
      "batch 160/ 2342, loss training: 1.8523\n",
      "batch 161/ 2342, loss training: 1.8756\n",
      "batch 162/ 2342, loss training: 1.9001\n",
      "batch 163/ 2342, loss training: 1.9945\n",
      "batch 164/ 2342, loss training: 2.1016\n",
      "batch 165/ 2342, loss training: 2.0110\n",
      "batch 166/ 2342, loss training: 1.8243\n",
      "batch 167/ 2342, loss training: 1.9158\n",
      "batch 168/ 2342, loss training: 2.0064\n",
      "batch 169/ 2342, loss training: 1.8759\n",
      "batch 170/ 2342, loss training: 1.8650\n",
      "batch 171/ 2342, loss training: 1.9090\n",
      "batch 172/ 2342, loss training: 2.1130\n",
      "batch 173/ 2342, loss training: 1.8321\n",
      "batch 174/ 2342, loss training: 1.9138\n",
      "batch 175/ 2342, loss training: 2.0296\n",
      "batch 176/ 2342, loss training: 1.9979\n",
      "batch 177/ 2342, loss training: 2.0332\n",
      "batch 178/ 2342, loss training: 1.7259\n",
      "batch 179/ 2342, loss training: 1.9588\n",
      "batch 180/ 2342, loss training: 1.9781\n",
      "batch 181/ 2342, loss training: 1.8362\n",
      "batch 182/ 2342, loss training: 1.8087\n",
      "batch 183/ 2342, loss training: 1.8493\n",
      "batch 184/ 2342, loss training: 2.0151\n",
      "batch 185/ 2342, loss training: 1.8733\n",
      "batch 186/ 2342, loss training: 1.6509\n",
      "batch 187/ 2342, loss training: 1.7902\n",
      "batch 188/ 2342, loss training: 1.8840\n",
      "batch 189/ 2342, loss training: 1.8778\n",
      "batch 190/ 2342, loss training: 1.9050\n",
      "batch 191/ 2342, loss training: 1.8449\n",
      "batch 192/ 2342, loss training: 1.9071\n",
      "batch 193/ 2342, loss training: 1.7478\n",
      "batch 194/ 2342, loss training: 1.7997\n",
      "batch 195/ 2342, loss training: 1.8921\n",
      "batch 196/ 2342, loss training: 1.9153\n",
      "batch 197/ 2342, loss training: 2.0853\n",
      "batch 198/ 2342, loss training: 1.8740\n",
      "batch 199/ 2342, loss training: 1.7165\n",
      "batch 200/ 2342, loss training: 1.9118\n",
      "batch 201/ 2342, loss training: 1.8343\n",
      "batch 202/ 2342, loss training: 1.8186\n",
      "batch 203/ 2342, loss training: 1.7879\n",
      "batch 204/ 2342, loss training: 1.9684\n",
      "batch 205/ 2342, loss training: 1.8429\n",
      "batch 206/ 2342, loss training: 1.8783\n",
      "batch 207/ 2342, loss training: 2.0173\n",
      "batch 208/ 2342, loss training: 1.7928\n",
      "batch 209/ 2342, loss training: 1.7133\n",
      "batch 210/ 2342, loss training: 1.8293\n",
      "batch 211/ 2342, loss training: 1.7167\n",
      "batch 212/ 2342, loss training: 1.8141\n",
      "batch 213/ 2342, loss training: 1.7033\n",
      "batch 214/ 2342, loss training: 1.8667\n",
      "batch 215/ 2342, loss training: 1.8919\n",
      "batch 216/ 2342, loss training: 1.8401\n",
      "batch 217/ 2342, loss training: 1.8106\n",
      "batch 218/ 2342, loss training: 1.7855\n",
      "batch 219/ 2342, loss training: 1.7651\n",
      "batch 220/ 2342, loss training: 1.8904\n",
      "batch 221/ 2342, loss training: 1.8499\n",
      "batch 222/ 2342, loss training: 1.9268\n",
      "batch 223/ 2342, loss training: 1.8392\n",
      "batch 224/ 2342, loss training: 1.8288\n",
      "batch 225/ 2342, loss training: 1.6527\n",
      "batch 226/ 2342, loss training: 1.7377\n",
      "batch 227/ 2342, loss training: 1.7774\n",
      "batch 228/ 2342, loss training: 1.7246\n",
      "batch 229/ 2342, loss training: 1.7104\n",
      "batch 230/ 2342, loss training: 1.7267\n",
      "batch 231/ 2342, loss training: 1.8389\n",
      "batch 232/ 2342, loss training: 1.7897\n",
      "batch 233/ 2342, loss training: 1.8014\n",
      "batch 234/ 2342, loss training: 1.7330\n",
      "batch 235/ 2342, loss training: 1.7172\n",
      "batch 236/ 2342, loss training: 1.7908\n",
      "batch 237/ 2342, loss training: 1.6897\n",
      "batch 238/ 2342, loss training: 1.6938\n",
      "batch 239/ 2342, loss training: 1.9014\n",
      "batch 240/ 2342, loss training: 1.7666\n",
      "batch 241/ 2342, loss training: 1.6022\n",
      "batch 242/ 2342, loss training: 1.6285\n",
      "batch 243/ 2342, loss training: 1.8980\n",
      "batch 244/ 2342, loss training: 1.7634\n",
      "batch 245/ 2342, loss training: 1.8898\n",
      "batch 246/ 2342, loss training: 1.7692\n",
      "batch 247/ 2342, loss training: 1.6657\n",
      "batch 248/ 2342, loss training: 1.5911\n",
      "batch 249/ 2342, loss training: 1.7264\n",
      "batch 250/ 2342, loss training: 1.8063\n",
      "batch 251/ 2342, loss training: 1.7763\n",
      "batch 252/ 2342, loss training: 1.8287\n",
      "batch 253/ 2342, loss training: 1.6809\n",
      "batch 254/ 2342, loss training: 1.6621\n",
      "batch 255/ 2342, loss training: 1.6177\n",
      "batch 256/ 2342, loss training: 1.6431\n",
      "batch 257/ 2342, loss training: 1.6448\n",
      "batch 258/ 2342, loss training: 1.6742\n",
      "batch 259/ 2342, loss training: 1.8240\n",
      "batch 260/ 2342, loss training: 1.6005\n",
      "batch 261/ 2342, loss training: 1.5149\n",
      "batch 262/ 2342, loss training: 1.6657\n",
      "batch 263/ 2342, loss training: 1.7971\n",
      "batch 264/ 2342, loss training: 1.6510\n",
      "batch 265/ 2342, loss training: 1.7004\n",
      "batch 266/ 2342, loss training: 1.5670\n",
      "batch 267/ 2342, loss training: 1.6133\n",
      "batch 268/ 2342, loss training: 1.7214\n",
      "batch 269/ 2342, loss training: 1.7076\n",
      "batch 270/ 2342, loss training: 1.5515\n",
      "batch 271/ 2342, loss training: 1.7175\n",
      "batch 272/ 2342, loss training: 1.7096\n",
      "batch 273/ 2342, loss training: 1.5417\n",
      "batch 274/ 2342, loss training: 1.6630\n",
      "batch 275/ 2342, loss training: 1.6672\n",
      "batch 276/ 2342, loss training: 1.6404\n",
      "batch 277/ 2342, loss training: 1.8347\n",
      "batch 278/ 2342, loss training: 1.6828\n",
      "batch 279/ 2342, loss training: 1.7265\n",
      "batch 280/ 2342, loss training: 1.7136\n",
      "batch 281/ 2342, loss training: 1.7441\n",
      "batch 282/ 2342, loss training: 1.5566\n",
      "batch 283/ 2342, loss training: 1.6856\n",
      "batch 284/ 2342, loss training: 1.7386\n",
      "batch 285/ 2342, loss training: 1.6503\n",
      "batch 286/ 2342, loss training: 1.7392\n",
      "batch 287/ 2342, loss training: 1.5575\n",
      "batch 288/ 2342, loss training: 1.7591\n",
      "batch 289/ 2342, loss training: 1.7887\n",
      "batch 290/ 2342, loss training: 1.7742\n",
      "batch 291/ 2342, loss training: 1.5841\n",
      "batch 292/ 2342, loss training: 1.6882\n",
      "batch 293/ 2342, loss training: 1.6465\n",
      "batch 294/ 2342, loss training: 1.7259\n",
      "batch 295/ 2342, loss training: 1.7534\n",
      "batch 296/ 2342, loss training: 1.5881\n",
      "batch 297/ 2342, loss training: 1.8087\n",
      "batch 298/ 2342, loss training: 1.7109\n",
      "batch 299/ 2342, loss training: 1.6547\n",
      "batch 300/ 2342, loss training: 1.6121\n",
      "batch 301/ 2342, loss training: 1.7574\n",
      "batch 302/ 2342, loss training: 1.7019\n",
      "batch 303/ 2342, loss training: 1.5687\n",
      "batch 304/ 2342, loss training: 1.6904\n",
      "batch 305/ 2342, loss training: 1.7370\n",
      "batch 306/ 2342, loss training: 1.5171\n",
      "batch 307/ 2342, loss training: 1.5068\n",
      "batch 308/ 2342, loss training: 1.6882\n",
      "batch 309/ 2342, loss training: 1.6631\n",
      "batch 310/ 2342, loss training: 1.6223\n",
      "batch 311/ 2342, loss training: 1.6221\n",
      "batch 312/ 2342, loss training: 1.6638\n",
      "batch 313/ 2342, loss training: 1.6696\n",
      "batch 314/ 2342, loss training: 1.6406\n",
      "batch 315/ 2342, loss training: 1.5598\n",
      "batch 316/ 2342, loss training: 1.5399\n",
      "batch 317/ 2342, loss training: 1.7011\n",
      "batch 318/ 2342, loss training: 1.6351\n",
      "batch 319/ 2342, loss training: 1.6258\n",
      "batch 320/ 2342, loss training: 1.6226\n",
      "batch 321/ 2342, loss training: 1.7249\n",
      "batch 322/ 2342, loss training: 1.5999\n",
      "batch 323/ 2342, loss training: 1.4705\n",
      "batch 324/ 2342, loss training: 1.7550\n",
      "batch 325/ 2342, loss training: 1.6662\n",
      "batch 326/ 2342, loss training: 1.7030\n",
      "batch 327/ 2342, loss training: 1.5830\n",
      "batch 328/ 2342, loss training: 1.6010\n",
      "batch 329/ 2342, loss training: 1.5573\n",
      "batch 330/ 2342, loss training: 1.5717\n",
      "batch 331/ 2342, loss training: 1.6510\n",
      "batch 332/ 2342, loss training: 1.5691\n",
      "batch 333/ 2342, loss training: 1.6347\n",
      "batch 334/ 2342, loss training: 1.4902\n",
      "batch 335/ 2342, loss training: 1.7266\n",
      "batch 336/ 2342, loss training: 1.5122\n",
      "batch 337/ 2342, loss training: 1.8076\n",
      "batch 338/ 2342, loss training: 1.7167\n",
      "batch 339/ 2342, loss training: 1.7033\n",
      "batch 340/ 2342, loss training: 1.5130\n",
      "batch 341/ 2342, loss training: 1.5496\n",
      "batch 342/ 2342, loss training: 1.3908\n",
      "batch 343/ 2342, loss training: 1.5441\n",
      "batch 344/ 2342, loss training: 1.5927\n",
      "batch 345/ 2342, loss training: 1.6959\n",
      "batch 346/ 2342, loss training: 1.4614\n",
      "batch 347/ 2342, loss training: 1.6009\n",
      "batch 348/ 2342, loss training: 1.8413\n",
      "batch 349/ 2342, loss training: 1.5965\n",
      "batch 350/ 2342, loss training: 1.5329\n",
      "batch 351/ 2342, loss training: 1.6192\n",
      "batch 352/ 2342, loss training: 1.7443\n",
      "batch 353/ 2342, loss training: 1.7060\n",
      "batch 354/ 2342, loss training: 1.5014\n",
      "batch 355/ 2342, loss training: 1.6518\n",
      "batch 356/ 2342, loss training: 1.5095\n",
      "batch 357/ 2342, loss training: 1.6091\n",
      "batch 358/ 2342, loss training: 1.5601\n",
      "batch 359/ 2342, loss training: 1.6833\n",
      "batch 360/ 2342, loss training: 1.5725\n",
      "batch 361/ 2342, loss training: 1.7622\n",
      "batch 362/ 2342, loss training: 1.5770\n",
      "batch 363/ 2342, loss training: 1.5531\n",
      "batch 364/ 2342, loss training: 1.6136\n",
      "batch 365/ 2342, loss training: 1.4833\n",
      "batch 366/ 2342, loss training: 1.5303\n",
      "batch 367/ 2342, loss training: 1.5394\n",
      "batch 368/ 2342, loss training: 1.5610\n",
      "batch 369/ 2342, loss training: 1.6181\n",
      "batch 370/ 2342, loss training: 1.6081\n",
      "batch 371/ 2342, loss training: 1.5268\n",
      "batch 372/ 2342, loss training: 1.5391\n",
      "batch 373/ 2342, loss training: 1.6448\n",
      "batch 374/ 2342, loss training: 1.6568\n",
      "batch 375/ 2342, loss training: 1.5213\n",
      "batch 376/ 2342, loss training: 1.5266\n",
      "batch 377/ 2342, loss training: 1.4817\n",
      "batch 378/ 2342, loss training: 1.6076\n",
      "batch 379/ 2342, loss training: 1.6389\n",
      "batch 380/ 2342, loss training: 1.5361\n",
      "batch 381/ 2342, loss training: 1.4839\n",
      "batch 382/ 2342, loss training: 1.6354\n",
      "batch 383/ 2342, loss training: 1.6260\n",
      "batch 384/ 2342, loss training: 1.5217\n",
      "batch 385/ 2342, loss training: 1.5859\n",
      "batch 386/ 2342, loss training: 1.5857\n",
      "batch 387/ 2342, loss training: 1.6515\n",
      "batch 388/ 2342, loss training: 1.5274\n",
      "batch 389/ 2342, loss training: 1.5616\n",
      "batch 390/ 2342, loss training: 1.6018\n",
      "batch 391/ 2342, loss training: 1.5161\n",
      "batch 392/ 2342, loss training: 1.5221\n",
      "batch 393/ 2342, loss training: 1.4073\n",
      "batch 394/ 2342, loss training: 1.5184\n",
      "batch 395/ 2342, loss training: 1.4733\n",
      "batch 396/ 2342, loss training: 1.5502\n",
      "batch 397/ 2342, loss training: 1.5924\n",
      "batch 398/ 2342, loss training: 1.5887\n",
      "batch 399/ 2342, loss training: 1.4426\n",
      "batch 400/ 2342, loss training: 1.5936\n",
      "batch 401/ 2342, loss training: 1.5765\n",
      "batch 402/ 2342, loss training: 1.5093\n",
      "batch 403/ 2342, loss training: 1.5264\n",
      "batch 404/ 2342, loss training: 1.6163\n",
      "batch 405/ 2342, loss training: 1.5326\n",
      "batch 406/ 2342, loss training: 1.6717\n",
      "batch 407/ 2342, loss training: 1.3530\n",
      "batch 408/ 2342, loss training: 1.6084\n",
      "batch 409/ 2342, loss training: 1.5684\n",
      "batch 410/ 2342, loss training: 1.5769\n",
      "batch 411/ 2342, loss training: 1.4657\n",
      "batch 412/ 2342, loss training: 1.4767\n",
      "batch 413/ 2342, loss training: 1.5595\n",
      "batch 414/ 2342, loss training: 1.5172\n",
      "batch 415/ 2342, loss training: 1.5052\n",
      "batch 416/ 2342, loss training: 1.4495\n",
      "batch 417/ 2342, loss training: 1.5944\n",
      "batch 418/ 2342, loss training: 1.5471\n",
      "batch 419/ 2342, loss training: 1.5173\n",
      "batch 420/ 2342, loss training: 1.5054\n",
      "batch 421/ 2342, loss training: 1.5605\n",
      "batch 422/ 2342, loss training: 1.7203\n",
      "batch 423/ 2342, loss training: 1.3864\n",
      "batch 424/ 2342, loss training: 1.6315\n",
      "batch 425/ 2342, loss training: 1.6167\n",
      "batch 426/ 2342, loss training: 1.6128\n",
      "batch 427/ 2342, loss training: 1.5722\n",
      "batch 428/ 2342, loss training: 1.4509\n",
      "batch 429/ 2342, loss training: 1.4343\n",
      "batch 430/ 2342, loss training: 1.5474\n",
      "batch 431/ 2342, loss training: 1.6092\n",
      "batch 432/ 2342, loss training: 1.5538\n",
      "batch 433/ 2342, loss training: 1.3954\n",
      "batch 434/ 2342, loss training: 1.5627\n",
      "batch 435/ 2342, loss training: 1.4729\n",
      "batch 436/ 2342, loss training: 1.6729\n",
      "batch 437/ 2342, loss training: 1.5661\n",
      "batch 438/ 2342, loss training: 1.5349\n",
      "batch 439/ 2342, loss training: 1.4690\n",
      "batch 440/ 2342, loss training: 1.5769\n",
      "batch 441/ 2342, loss training: 1.4561\n",
      "batch 442/ 2342, loss training: 1.4290\n",
      "batch 443/ 2342, loss training: 1.5556\n",
      "batch 444/ 2342, loss training: 1.3890\n",
      "batch 445/ 2342, loss training: 1.6191\n",
      "batch 446/ 2342, loss training: 1.4047\n",
      "batch 447/ 2342, loss training: 1.5756\n",
      "batch 448/ 2342, loss training: 1.3709\n",
      "batch 449/ 2342, loss training: 1.3832\n",
      "batch 450/ 2342, loss training: 1.4531\n",
      "batch 451/ 2342, loss training: 1.3952\n",
      "batch 452/ 2342, loss training: 1.5200\n",
      "batch 453/ 2342, loss training: 1.4056\n",
      "batch 454/ 2342, loss training: 1.3360\n",
      "batch 455/ 2342, loss training: 1.5967\n",
      "batch 456/ 2342, loss training: 1.4821\n",
      "batch 457/ 2342, loss training: 1.5318\n",
      "batch 458/ 2342, loss training: 1.4662\n",
      "batch 459/ 2342, loss training: 1.5143\n",
      "batch 460/ 2342, loss training: 1.5368\n",
      "batch 461/ 2342, loss training: 1.4940\n",
      "batch 462/ 2342, loss training: 1.4070\n",
      "batch 463/ 2342, loss training: 1.4473\n",
      "batch 464/ 2342, loss training: 1.5440\n",
      "batch 465/ 2342, loss training: 1.4481\n",
      "batch 466/ 2342, loss training: 1.4212\n",
      "batch 467/ 2342, loss training: 1.4497\n",
      "batch 468/ 2342, loss training: 1.3150\n",
      "batch 469/ 2342, loss training: 1.4548\n",
      "batch 470/ 2342, loss training: 1.5228\n",
      "batch 471/ 2342, loss training: 1.5678\n",
      "batch 472/ 2342, loss training: 1.4320\n",
      "batch 473/ 2342, loss training: 1.4896\n",
      "batch 474/ 2342, loss training: 1.3906\n",
      "batch 475/ 2342, loss training: 1.3051\n",
      "batch 476/ 2342, loss training: 1.5255\n",
      "batch 477/ 2342, loss training: 1.3783\n",
      "batch 478/ 2342, loss training: 1.3752\n",
      "batch 479/ 2342, loss training: 1.5533\n",
      "batch 480/ 2342, loss training: 1.5938\n",
      "batch 481/ 2342, loss training: 1.4476\n",
      "batch 482/ 2342, loss training: 1.4978\n",
      "batch 483/ 2342, loss training: 1.3917\n",
      "batch 484/ 2342, loss training: 1.5921\n",
      "batch 485/ 2342, loss training: 1.3856\n",
      "batch 486/ 2342, loss training: 1.4701\n",
      "batch 487/ 2342, loss training: 1.5152\n",
      "batch 488/ 2342, loss training: 1.4827\n",
      "batch 489/ 2342, loss training: 1.4364\n",
      "batch 490/ 2342, loss training: 1.3689\n",
      "batch 491/ 2342, loss training: 1.3900\n",
      "batch 492/ 2342, loss training: 1.4239\n",
      "batch 493/ 2342, loss training: 1.4886\n",
      "batch 494/ 2342, loss training: 1.4373\n",
      "batch 495/ 2342, loss training: 1.4876\n",
      "batch 496/ 2342, loss training: 1.4673\n",
      "batch 497/ 2342, loss training: 1.4666\n",
      "batch 498/ 2342, loss training: 1.4553\n",
      "batch 499/ 2342, loss training: 1.4371\n",
      "batch 500/ 2342, loss training: 1.4124\n",
      "batch 501/ 2342, loss training: 1.3404\n",
      "batch 502/ 2342, loss training: 1.5369\n",
      "batch 503/ 2342, loss training: 1.3707\n",
      "batch 504/ 2342, loss training: 1.3849\n",
      "batch 505/ 2342, loss training: 1.3808\n",
      "batch 506/ 2342, loss training: 1.4339\n",
      "batch 507/ 2342, loss training: 1.3727\n",
      "batch 508/ 2342, loss training: 1.4074\n",
      "batch 509/ 2342, loss training: 1.2816\n",
      "batch 510/ 2342, loss training: 1.4831\n",
      "batch 511/ 2342, loss training: 1.4023\n",
      "batch 512/ 2342, loss training: 1.3952\n",
      "batch 513/ 2342, loss training: 1.4680\n",
      "batch 514/ 2342, loss training: 1.5114\n",
      "batch 515/ 2342, loss training: 1.5239\n",
      "batch 516/ 2342, loss training: 1.4651\n",
      "batch 517/ 2342, loss training: 1.5543\n",
      "batch 518/ 2342, loss training: 1.4791\n",
      "batch 519/ 2342, loss training: 1.4450\n",
      "batch 520/ 2342, loss training: 1.4239\n",
      "batch 521/ 2342, loss training: 1.2909\n",
      "batch 522/ 2342, loss training: 1.3355\n",
      "batch 523/ 2342, loss training: 1.3499\n",
      "batch 524/ 2342, loss training: 1.3749\n",
      "batch 525/ 2342, loss training: 1.3754\n",
      "batch 526/ 2342, loss training: 1.4157\n",
      "batch 527/ 2342, loss training: 1.4226\n",
      "batch 528/ 2342, loss training: 1.3607\n",
      "batch 529/ 2342, loss training: 1.4091\n",
      "batch 530/ 2342, loss training: 1.2725\n",
      "batch 531/ 2342, loss training: 1.4285\n",
      "batch 532/ 2342, loss training: 1.3801\n",
      "batch 533/ 2342, loss training: 1.2898\n",
      "batch 534/ 2342, loss training: 1.4604\n",
      "batch 535/ 2342, loss training: 1.3923\n",
      "batch 536/ 2342, loss training: 1.4187\n",
      "batch 537/ 2342, loss training: 1.2794\n",
      "batch 538/ 2342, loss training: 1.3505\n",
      "batch 539/ 2342, loss training: 1.3953\n",
      "batch 540/ 2342, loss training: 1.2269\n",
      "batch 541/ 2342, loss training: 1.4684\n",
      "batch 542/ 2342, loss training: 1.5233\n",
      "batch 543/ 2342, loss training: 1.4846\n",
      "batch 544/ 2342, loss training: 1.4671\n",
      "batch 545/ 2342, loss training: 1.3785\n",
      "batch 546/ 2342, loss training: 1.3623\n",
      "batch 547/ 2342, loss training: 1.2890\n",
      "batch 548/ 2342, loss training: 1.3318\n",
      "batch 549/ 2342, loss training: 1.4714\n",
      "batch 550/ 2342, loss training: 1.3336\n",
      "batch 551/ 2342, loss training: 1.3375\n",
      "batch 552/ 2342, loss training: 1.3048\n",
      "batch 553/ 2342, loss training: 1.3217\n",
      "batch 554/ 2342, loss training: 1.3304\n",
      "batch 555/ 2342, loss training: 1.3084\n",
      "batch 556/ 2342, loss training: 1.2321\n",
      "batch 557/ 2342, loss training: 1.1353\n",
      "batch 558/ 2342, loss training: 1.1469\n",
      "batch 559/ 2342, loss training: 1.3184\n",
      "batch 560/ 2342, loss training: 1.4201\n",
      "batch 561/ 2342, loss training: 1.2617\n",
      "batch 562/ 2342, loss training: 1.3477\n",
      "batch 563/ 2342, loss training: 1.3214\n",
      "batch 564/ 2342, loss training: 1.3780\n",
      "batch 565/ 2342, loss training: 1.3810\n",
      "batch 566/ 2342, loss training: 1.4138\n",
      "batch 567/ 2342, loss training: 1.4334\n",
      "batch 568/ 2342, loss training: 1.3284\n",
      "batch 569/ 2342, loss training: 1.3412\n",
      "batch 570/ 2342, loss training: 1.4258\n",
      "batch 571/ 2342, loss training: 1.2630\n",
      "batch 572/ 2342, loss training: 1.3357\n",
      "batch 573/ 2342, loss training: 1.3557\n",
      "batch 574/ 2342, loss training: 1.2319\n",
      "batch 575/ 2342, loss training: 1.3100\n",
      "batch 576/ 2342, loss training: 1.4039\n",
      "batch 577/ 2342, loss training: 1.3275\n",
      "batch 578/ 2342, loss training: 1.5318\n",
      "batch 579/ 2342, loss training: 1.2166\n",
      "batch 580/ 2342, loss training: 1.2460\n",
      "batch 581/ 2342, loss training: 1.4393\n",
      "batch 582/ 2342, loss training: 1.3565\n",
      "batch 583/ 2342, loss training: 1.3038\n",
      "batch 584/ 2342, loss training: 1.2030\n",
      "batch 585/ 2342, loss training: 1.3561\n",
      "batch 586/ 2342, loss training: 1.3385\n",
      "batch 587/ 2342, loss training: 1.2538\n",
      "batch 588/ 2342, loss training: 1.3330\n",
      "batch 589/ 2342, loss training: 1.3644\n",
      "batch 590/ 2342, loss training: 1.3447\n",
      "batch 591/ 2342, loss training: 1.2772\n",
      "batch 592/ 2342, loss training: 1.2428\n",
      "batch 593/ 2342, loss training: 1.2220\n",
      "batch 594/ 2342, loss training: 1.3962\n",
      "batch 595/ 2342, loss training: 1.4447\n",
      "batch 596/ 2342, loss training: 1.2473\n",
      "batch 597/ 2342, loss training: 1.3829\n",
      "batch 598/ 2342, loss training: 1.3579\n",
      "batch 599/ 2342, loss training: 1.3520\n",
      "batch 600/ 2342, loss training: 1.3457\n",
      "batch 601/ 2342, loss training: 1.2312\n",
      "batch 602/ 2342, loss training: 1.2520\n",
      "batch 603/ 2342, loss training: 1.3224\n",
      "batch 604/ 2342, loss training: 1.2658\n",
      "batch 605/ 2342, loss training: 1.1869\n",
      "batch 606/ 2342, loss training: 1.4566\n",
      "batch 607/ 2342, loss training: 1.3141\n",
      "batch 608/ 2342, loss training: 1.2520\n",
      "batch 609/ 2342, loss training: 1.2575\n",
      "batch 610/ 2342, loss training: 1.4036\n",
      "batch 611/ 2342, loss training: 1.3370\n",
      "batch 612/ 2342, loss training: 1.3100\n",
      "batch 613/ 2342, loss training: 1.3570\n",
      "batch 614/ 2342, loss training: 1.1091\n",
      "batch 615/ 2342, loss training: 1.2835\n",
      "batch 616/ 2342, loss training: 1.3811\n",
      "batch 617/ 2342, loss training: 1.2483\n",
      "batch 618/ 2342, loss training: 1.2753\n",
      "batch 619/ 2342, loss training: 1.3386\n",
      "batch 620/ 2342, loss training: 1.3420\n",
      "batch 621/ 2342, loss training: 1.2036\n",
      "batch 622/ 2342, loss training: 1.5159\n",
      "batch 623/ 2342, loss training: 1.2936\n",
      "batch 624/ 2342, loss training: 1.2444\n",
      "batch 625/ 2342, loss training: 1.3114\n",
      "batch 626/ 2342, loss training: 1.1936\n",
      "batch 627/ 2342, loss training: 1.1545\n",
      "batch 628/ 2342, loss training: 1.2378\n",
      "batch 629/ 2342, loss training: 1.3088\n",
      "batch 630/ 2342, loss training: 1.2875\n",
      "batch 631/ 2342, loss training: 1.2876\n",
      "batch 632/ 2342, loss training: 1.3814\n",
      "batch 633/ 2342, loss training: 1.2761\n",
      "batch 634/ 2342, loss training: 1.4316\n",
      "batch 635/ 2342, loss training: 1.3904\n",
      "batch 636/ 2342, loss training: 1.3109\n",
      "batch 637/ 2342, loss training: 1.3674\n",
      "batch 638/ 2342, loss training: 1.2313\n",
      "batch 639/ 2342, loss training: 1.2220\n",
      "batch 640/ 2342, loss training: 1.3396\n",
      "batch 641/ 2342, loss training: 1.2228\n",
      "batch 642/ 2342, loss training: 1.2282\n",
      "batch 643/ 2342, loss training: 1.3010\n",
      "batch 644/ 2342, loss training: 1.1796\n",
      "batch 645/ 2342, loss training: 1.2663\n",
      "batch 646/ 2342, loss training: 1.3236\n",
      "batch 647/ 2342, loss training: 1.1755\n",
      "batch 648/ 2342, loss training: 1.2225\n",
      "batch 649/ 2342, loss training: 1.2103\n",
      "batch 650/ 2342, loss training: 1.1751\n",
      "batch 651/ 2342, loss training: 1.0427\n",
      "batch 652/ 2342, loss training: 1.3036\n",
      "batch 653/ 2342, loss training: 1.2926\n",
      "batch 654/ 2342, loss training: 1.4136\n",
      "batch 655/ 2342, loss training: 1.2566\n",
      "batch 656/ 2342, loss training: 1.2810\n",
      "batch 657/ 2342, loss training: 1.2657\n",
      "batch 658/ 2342, loss training: 1.2590\n",
      "batch 659/ 2342, loss training: 1.1302\n",
      "batch 660/ 2342, loss training: 1.2641\n",
      "batch 661/ 2342, loss training: 1.2910\n",
      "batch 662/ 2342, loss training: 1.2021\n",
      "batch 663/ 2342, loss training: 1.1710\n",
      "batch 664/ 2342, loss training: 1.2365\n",
      "batch 665/ 2342, loss training: 1.3063\n",
      "batch 666/ 2342, loss training: 1.1428\n",
      "batch 667/ 2342, loss training: 1.1419\n",
      "batch 668/ 2342, loss training: 1.1491\n",
      "batch 669/ 2342, loss training: 1.1298\n",
      "batch 670/ 2342, loss training: 1.1425\n",
      "batch 671/ 2342, loss training: 1.1329\n",
      "batch 672/ 2342, loss training: 1.3536\n",
      "batch 673/ 2342, loss training: 1.1936\n",
      "batch 674/ 2342, loss training: 1.1771\n",
      "batch 675/ 2342, loss training: 1.2412\n",
      "batch 676/ 2342, loss training: 1.2224\n",
      "batch 677/ 2342, loss training: 1.2410\n",
      "batch 678/ 2342, loss training: 1.2011\n",
      "batch 679/ 2342, loss training: 1.1871\n",
      "batch 680/ 2342, loss training: 1.2412\n",
      "batch 681/ 2342, loss training: 1.2465\n",
      "batch 682/ 2342, loss training: 1.4171\n",
      "batch 683/ 2342, loss training: 1.1776\n",
      "batch 684/ 2342, loss training: 1.1413\n",
      "batch 685/ 2342, loss training: 1.1488\n",
      "batch 686/ 2342, loss training: 1.2364\n",
      "batch 687/ 2342, loss training: 1.3190\n",
      "batch 688/ 2342, loss training: 1.2899\n",
      "batch 689/ 2342, loss training: 1.2513\n",
      "batch 690/ 2342, loss training: 1.2166\n",
      "batch 691/ 2342, loss training: 1.3347\n",
      "batch 692/ 2342, loss training: 1.1708\n",
      "batch 693/ 2342, loss training: 1.3785\n",
      "batch 694/ 2342, loss training: 1.1907\n",
      "batch 695/ 2342, loss training: 1.0564\n",
      "batch 696/ 2342, loss training: 1.1925\n",
      "batch 697/ 2342, loss training: 1.2591\n",
      "batch 698/ 2342, loss training: 1.2001\n",
      "batch 699/ 2342, loss training: 1.3800\n",
      "batch 700/ 2342, loss training: 1.2218\n",
      "batch 701/ 2342, loss training: 1.2367\n",
      "batch 702/ 2342, loss training: 1.1661\n",
      "batch 703/ 2342, loss training: 1.2368\n",
      "batch 704/ 2342, loss training: 1.2124\n",
      "batch 705/ 2342, loss training: 1.2727\n",
      "batch 706/ 2342, loss training: 1.1352\n",
      "batch 707/ 2342, loss training: 1.2286\n",
      "batch 708/ 2342, loss training: 1.2696\n",
      "batch 709/ 2342, loss training: 1.0940\n",
      "batch 710/ 2342, loss training: 1.2191\n",
      "batch 711/ 2342, loss training: 1.1926\n",
      "batch 712/ 2342, loss training: 1.2527\n",
      "batch 713/ 2342, loss training: 1.1788\n",
      "batch 714/ 2342, loss training: 1.1844\n",
      "batch 715/ 2342, loss training: 1.2583\n",
      "batch 716/ 2342, loss training: 1.1576\n",
      "batch 717/ 2342, loss training: 1.3337\n",
      "batch 718/ 2342, loss training: 1.2541\n",
      "batch 719/ 2342, loss training: 1.1123\n",
      "batch 720/ 2342, loss training: 1.2050\n",
      "batch 721/ 2342, loss training: 1.1872\n",
      "batch 722/ 2342, loss training: 1.1342\n",
      "batch 723/ 2342, loss training: 1.0677\n",
      "batch 724/ 2342, loss training: 1.2224\n",
      "batch 725/ 2342, loss training: 1.2939\n",
      "batch 726/ 2342, loss training: 1.1636\n",
      "batch 727/ 2342, loss training: 1.2587\n",
      "batch 728/ 2342, loss training: 1.0826\n",
      "batch 729/ 2342, loss training: 1.2321\n",
      "batch 730/ 2342, loss training: 1.2207\n",
      "batch 731/ 2342, loss training: 1.2439\n",
      "batch 732/ 2342, loss training: 1.1648\n",
      "batch 733/ 2342, loss training: 1.1453\n",
      "batch 734/ 2342, loss training: 1.3489\n",
      "batch 735/ 2342, loss training: 1.2277\n",
      "batch 736/ 2342, loss training: 1.2716\n",
      "batch 737/ 2342, loss training: 1.1848\n",
      "batch 738/ 2342, loss training: 1.1453\n",
      "batch 739/ 2342, loss training: 1.2116\n",
      "batch 740/ 2342, loss training: 1.3547\n",
      "batch 741/ 2342, loss training: 1.0865\n",
      "batch 742/ 2342, loss training: 1.2074\n",
      "batch 743/ 2342, loss training: 1.2106\n",
      "batch 744/ 2342, loss training: 1.1803\n",
      "batch 745/ 2342, loss training: 1.0261\n",
      "batch 746/ 2342, loss training: 1.2035\n",
      "batch 747/ 2342, loss training: 1.1699\n",
      "batch 748/ 2342, loss training: 1.2203\n",
      "batch 749/ 2342, loss training: 1.1248\n",
      "batch 750/ 2342, loss training: 1.0312\n",
      "batch 751/ 2342, loss training: 1.1988\n",
      "batch 752/ 2342, loss training: 1.3110\n",
      "batch 753/ 2342, loss training: 1.0479\n",
      "batch 754/ 2342, loss training: 1.2596\n",
      "batch 755/ 2342, loss training: 1.2465\n",
      "batch 756/ 2342, loss training: 1.1852\n",
      "batch 757/ 2342, loss training: 1.0957\n",
      "batch 758/ 2342, loss training: 1.0681\n",
      "batch 759/ 2342, loss training: 1.0752\n",
      "batch 760/ 2342, loss training: 1.1532\n",
      "batch 761/ 2342, loss training: 1.0666\n",
      "batch 762/ 2342, loss training: 1.1224\n",
      "batch 763/ 2342, loss training: 1.0176\n",
      "batch 764/ 2342, loss training: 1.1267\n",
      "batch 765/ 2342, loss training: 1.0029\n",
      "batch 766/ 2342, loss training: 1.1911\n",
      "batch 767/ 2342, loss training: 1.1219\n",
      "batch 768/ 2342, loss training: 1.0998\n",
      "batch 769/ 2342, loss training: 1.0965\n",
      "batch 770/ 2342, loss training: 1.1911\n",
      "batch 771/ 2342, loss training: 1.0828\n",
      "batch 772/ 2342, loss training: 1.2328\n",
      "batch 773/ 2342, loss training: 1.1421\n",
      "batch 774/ 2342, loss training: 1.0054\n",
      "batch 775/ 2342, loss training: 1.2986\n",
      "batch 776/ 2342, loss training: 1.0667\n",
      "batch 777/ 2342, loss training: 1.1286\n",
      "batch 778/ 2342, loss training: 1.1867\n",
      "batch 779/ 2342, loss training: 0.9733\n",
      "batch 780/ 2342, loss training: 1.0658\n",
      "batch 781/ 2342, loss training: 1.1829\n",
      "batch 782/ 2342, loss training: 1.0882\n",
      "batch 783/ 2342, loss training: 1.0237\n",
      "batch 784/ 2342, loss training: 1.1010\n",
      "batch 785/ 2342, loss training: 1.0507\n",
      "batch 786/ 2342, loss training: 1.0721\n",
      "batch 787/ 2342, loss training: 1.2712\n",
      "batch 788/ 2342, loss training: 1.0899\n",
      "batch 789/ 2342, loss training: 1.1662\n",
      "batch 790/ 2342, loss training: 1.1796\n",
      "batch 791/ 2342, loss training: 1.1444\n",
      "batch 792/ 2342, loss training: 1.1574\n",
      "batch 793/ 2342, loss training: 1.0444\n",
      "batch 794/ 2342, loss training: 1.1159\n",
      "batch 795/ 2342, loss training: 1.2087\n",
      "batch 796/ 2342, loss training: 1.2963\n",
      "batch 797/ 2342, loss training: 1.2632\n",
      "batch 798/ 2342, loss training: 1.0722\n",
      "batch 799/ 2342, loss training: 1.0891\n",
      "batch 800/ 2342, loss training: 1.1766\n",
      "batch 801/ 2342, loss training: 1.0695\n",
      "batch 802/ 2342, loss training: 1.0583\n",
      "batch 803/ 2342, loss training: 1.0434\n",
      "batch 804/ 2342, loss training: 1.1315\n",
      "batch 805/ 2342, loss training: 1.0593\n",
      "batch 806/ 2342, loss training: 1.1422\n",
      "batch 807/ 2342, loss training: 1.0873\n",
      "batch 808/ 2342, loss training: 1.0094\n",
      "batch 809/ 2342, loss training: 1.0571\n",
      "batch 810/ 2342, loss training: 1.0994\n",
      "batch 811/ 2342, loss training: 0.9751\n",
      "batch 812/ 2342, loss training: 1.0525\n",
      "batch 813/ 2342, loss training: 1.1160\n",
      "batch 814/ 2342, loss training: 0.9858\n",
      "batch 815/ 2342, loss training: 1.1419\n",
      "batch 816/ 2342, loss training: 0.9622\n",
      "batch 817/ 2342, loss training: 1.0910\n",
      "batch 818/ 2342, loss training: 1.1335\n",
      "batch 819/ 2342, loss training: 1.1987\n",
      "batch 820/ 2342, loss training: 1.1067\n",
      "batch 821/ 2342, loss training: 1.1865\n",
      "batch 822/ 2342, loss training: 1.1276\n",
      "batch 823/ 2342, loss training: 1.0502\n",
      "batch 824/ 2342, loss training: 1.1563\n",
      "batch 825/ 2342, loss training: 1.1574\n",
      "batch 826/ 2342, loss training: 1.1441\n",
      "batch 827/ 2342, loss training: 0.9379\n",
      "batch 828/ 2342, loss training: 1.0878\n",
      "batch 829/ 2342, loss training: 1.2790\n",
      "batch 830/ 2342, loss training: 1.1545\n",
      "batch 831/ 2342, loss training: 0.9607\n",
      "batch 832/ 2342, loss training: 1.0325\n",
      "batch 833/ 2342, loss training: 1.1311\n",
      "batch 834/ 2342, loss training: 1.1531\n",
      "batch 835/ 2342, loss training: 1.1066\n",
      "batch 836/ 2342, loss training: 1.1013\n",
      "batch 837/ 2342, loss training: 1.1339\n",
      "batch 838/ 2342, loss training: 1.0476\n",
      "batch 839/ 2342, loss training: 1.0124\n",
      "batch 840/ 2342, loss training: 1.1238\n",
      "batch 841/ 2342, loss training: 1.0006\n",
      "batch 842/ 2342, loss training: 1.0725\n",
      "batch 843/ 2342, loss training: 1.0935\n",
      "batch 844/ 2342, loss training: 1.1324\n",
      "batch 845/ 2342, loss training: 1.0551\n",
      "batch 846/ 2342, loss training: 0.9489\n",
      "batch 847/ 2342, loss training: 0.9374\n",
      "batch 848/ 2342, loss training: 0.9148\n",
      "batch 849/ 2342, loss training: 0.9986\n",
      "batch 850/ 2342, loss training: 1.0547\n",
      "batch 851/ 2342, loss training: 1.0688\n",
      "batch 852/ 2342, loss training: 0.9285\n",
      "batch 853/ 2342, loss training: 1.1874\n",
      "batch 854/ 2342, loss training: 1.0653\n",
      "batch 855/ 2342, loss training: 1.0603\n",
      "batch 856/ 2342, loss training: 1.1714\n",
      "batch 857/ 2342, loss training: 1.0888\n",
      "batch 858/ 2342, loss training: 1.1929\n",
      "batch 859/ 2342, loss training: 1.1602\n",
      "batch 860/ 2342, loss training: 1.0195\n",
      "batch 861/ 2342, loss training: 1.1936\n",
      "batch 862/ 2342, loss training: 1.2950\n",
      "batch 863/ 2342, loss training: 1.0765\n",
      "batch 864/ 2342, loss training: 1.0888\n",
      "batch 865/ 2342, loss training: 1.0624\n",
      "batch 866/ 2342, loss training: 1.0843\n",
      "batch 867/ 2342, loss training: 1.0498\n",
      "batch 868/ 2342, loss training: 1.1407\n",
      "batch 869/ 2342, loss training: 1.0851\n",
      "batch 870/ 2342, loss training: 1.0225\n",
      "batch 871/ 2342, loss training: 1.0487\n",
      "batch 872/ 2342, loss training: 0.9638\n",
      "batch 873/ 2342, loss training: 1.2407\n",
      "batch 874/ 2342, loss training: 1.0616\n",
      "batch 875/ 2342, loss training: 1.1174\n",
      "batch 876/ 2342, loss training: 1.0984\n",
      "batch 877/ 2342, loss training: 1.1523\n",
      "batch 878/ 2342, loss training: 1.1965\n",
      "batch 879/ 2342, loss training: 1.0109\n",
      "batch 880/ 2342, loss training: 1.1455\n",
      "batch 881/ 2342, loss training: 1.0982\n",
      "batch 882/ 2342, loss training: 1.0171\n",
      "batch 883/ 2342, loss training: 1.1254\n",
      "batch 884/ 2342, loss training: 1.0300\n",
      "batch 885/ 2342, loss training: 1.1360\n",
      "batch 886/ 2342, loss training: 0.9724\n",
      "batch 887/ 2342, loss training: 1.0197\n",
      "batch 888/ 2342, loss training: 1.0985\n",
      "batch 889/ 2342, loss training: 1.0240\n",
      "batch 890/ 2342, loss training: 1.0782\n",
      "batch 891/ 2342, loss training: 1.2110\n",
      "batch 892/ 2342, loss training: 1.0467\n",
      "batch 893/ 2342, loss training: 1.0754\n",
      "batch 894/ 2342, loss training: 0.9901\n",
      "batch 895/ 2342, loss training: 1.0856\n",
      "batch 896/ 2342, loss training: 0.9698\n",
      "batch 897/ 2342, loss training: 1.1246\n",
      "batch 898/ 2342, loss training: 0.8732\n",
      "batch 899/ 2342, loss training: 0.9191\n",
      "batch 900/ 2342, loss training: 1.0966\n",
      "batch 901/ 2342, loss training: 0.9219\n",
      "batch 902/ 2342, loss training: 0.9077\n",
      "batch 903/ 2342, loss training: 1.1742\n",
      "batch 904/ 2342, loss training: 1.0873\n",
      "batch 905/ 2342, loss training: 1.0455\n",
      "batch 906/ 2342, loss training: 1.0249\n",
      "batch 907/ 2342, loss training: 0.9963\n",
      "batch 908/ 2342, loss training: 1.1342\n",
      "batch 909/ 2342, loss training: 1.1941\n",
      "batch 910/ 2342, loss training: 1.0221\n",
      "batch 911/ 2342, loss training: 1.0306\n",
      "batch 912/ 2342, loss training: 1.0708\n",
      "batch 913/ 2342, loss training: 1.2100\n",
      "batch 914/ 2342, loss training: 1.1155\n",
      "batch 915/ 2342, loss training: 1.0559\n",
      "batch 916/ 2342, loss training: 1.0862\n",
      "batch 917/ 2342, loss training: 1.0901\n",
      "batch 918/ 2342, loss training: 1.0305\n",
      "batch 919/ 2342, loss training: 0.9446\n",
      "batch 920/ 2342, loss training: 1.0751\n",
      "batch 921/ 2342, loss training: 1.1455\n",
      "batch 922/ 2342, loss training: 0.9627\n",
      "batch 923/ 2342, loss training: 0.9877\n",
      "batch 924/ 2342, loss training: 1.0288\n",
      "batch 925/ 2342, loss training: 0.9003\n",
      "batch 926/ 2342, loss training: 1.1397\n",
      "batch 927/ 2342, loss training: 1.0469\n",
      "batch 928/ 2342, loss training: 0.9157\n",
      "batch 929/ 2342, loss training: 1.0828\n",
      "batch 930/ 2342, loss training: 1.0709\n",
      "batch 931/ 2342, loss training: 1.0879\n",
      "batch 932/ 2342, loss training: 0.9271\n",
      "batch 933/ 2342, loss training: 1.0347\n",
      "batch 934/ 2342, loss training: 0.9761\n",
      "batch 935/ 2342, loss training: 0.9278\n",
      "batch 936/ 2342, loss training: 0.9979\n",
      "batch 937/ 2342, loss training: 0.9907\n",
      "batch 938/ 2342, loss training: 0.9624\n",
      "batch 939/ 2342, loss training: 1.1405\n",
      "batch 940/ 2342, loss training: 0.9857\n",
      "batch 941/ 2342, loss training: 1.0196\n",
      "batch 942/ 2342, loss training: 1.1740\n",
      "batch 943/ 2342, loss training: 1.0406\n",
      "batch 944/ 2342, loss training: 0.9992\n",
      "batch 945/ 2342, loss training: 1.0970\n",
      "batch 946/ 2342, loss training: 0.9739\n",
      "batch 947/ 2342, loss training: 1.1395\n",
      "batch 948/ 2342, loss training: 1.0209\n",
      "batch 949/ 2342, loss training: 1.0757\n",
      "batch 950/ 2342, loss training: 0.9524\n",
      "batch 951/ 2342, loss training: 0.9000\n",
      "batch 952/ 2342, loss training: 0.9764\n",
      "batch 953/ 2342, loss training: 0.8963\n",
      "batch 954/ 2342, loss training: 1.0171\n",
      "batch 955/ 2342, loss training: 0.9233\n",
      "batch 956/ 2342, loss training: 1.0801\n",
      "batch 957/ 2342, loss training: 1.0798\n",
      "batch 958/ 2342, loss training: 0.8960\n",
      "batch 959/ 2342, loss training: 1.1212\n",
      "batch 960/ 2342, loss training: 1.0386\n",
      "batch 961/ 2342, loss training: 0.9564\n",
      "batch 962/ 2342, loss training: 0.8184\n",
      "batch 963/ 2342, loss training: 0.9594\n",
      "batch 964/ 2342, loss training: 0.8921\n",
      "batch 965/ 2342, loss training: 0.9880\n",
      "batch 966/ 2342, loss training: 1.0481\n",
      "batch 967/ 2342, loss training: 1.0880\n",
      "batch 968/ 2342, loss training: 1.1162\n",
      "batch 969/ 2342, loss training: 0.8396\n",
      "batch 970/ 2342, loss training: 0.9442\n",
      "batch 971/ 2342, loss training: 1.0224\n",
      "batch 972/ 2342, loss training: 1.0941\n",
      "batch 973/ 2342, loss training: 0.9479\n",
      "batch 974/ 2342, loss training: 0.9809\n",
      "batch 975/ 2342, loss training: 0.9069\n",
      "batch 976/ 2342, loss training: 1.0332\n",
      "batch 977/ 2342, loss training: 0.9347\n",
      "batch 978/ 2342, loss training: 0.8768\n",
      "batch 979/ 2342, loss training: 0.8980\n",
      "batch 980/ 2342, loss training: 1.0225\n",
      "batch 981/ 2342, loss training: 0.9967\n",
      "batch 982/ 2342, loss training: 1.1045\n",
      "batch 983/ 2342, loss training: 0.8245\n",
      "batch 984/ 2342, loss training: 1.0158\n",
      "batch 985/ 2342, loss training: 0.9127\n",
      "batch 986/ 2342, loss training: 0.9941\n",
      "batch 987/ 2342, loss training: 0.9394\n",
      "batch 988/ 2342, loss training: 0.9426\n",
      "batch 989/ 2342, loss training: 1.0826\n",
      "batch 990/ 2342, loss training: 1.1067\n",
      "batch 991/ 2342, loss training: 1.0776\n",
      "batch 992/ 2342, loss training: 1.0355\n",
      "batch 993/ 2342, loss training: 1.0056\n",
      "batch 994/ 2342, loss training: 1.0557\n",
      "batch 995/ 2342, loss training: 1.0859\n",
      "batch 996/ 2342, loss training: 1.1151\n",
      "batch 997/ 2342, loss training: 0.8901\n",
      "batch 998/ 2342, loss training: 0.8073\n",
      "batch 999/ 2342, loss training: 0.9512\n",
      "batch 1000/ 2342, loss training: 0.9340\n",
      "batch 1001/ 2342, loss training: 0.9001\n",
      "batch 1002/ 2342, loss training: 0.9946\n",
      "batch 1003/ 2342, loss training: 0.9231\n",
      "batch 1004/ 2342, loss training: 0.9592\n",
      "batch 1005/ 2342, loss training: 0.8475\n",
      "batch 1006/ 2342, loss training: 0.9643\n",
      "batch 1007/ 2342, loss training: 0.9617\n",
      "batch 1008/ 2342, loss training: 1.0008\n",
      "batch 1009/ 2342, loss training: 1.1253\n",
      "batch 1010/ 2342, loss training: 1.0495\n",
      "batch 1011/ 2342, loss training: 1.0073\n",
      "batch 1012/ 2342, loss training: 0.9940\n",
      "batch 1013/ 2342, loss training: 0.8532\n",
      "batch 1014/ 2342, loss training: 1.0072\n",
      "batch 1015/ 2342, loss training: 0.9858\n",
      "batch 1016/ 2342, loss training: 1.0282\n",
      "batch 1017/ 2342, loss training: 1.0571\n",
      "batch 1018/ 2342, loss training: 1.0418\n",
      "batch 1019/ 2342, loss training: 0.9031\n",
      "batch 1020/ 2342, loss training: 1.0496\n",
      "batch 1021/ 2342, loss training: 0.8370\n",
      "batch 1022/ 2342, loss training: 0.8752\n",
      "batch 1023/ 2342, loss training: 1.0162\n",
      "batch 1024/ 2342, loss training: 1.0267\n",
      "batch 1025/ 2342, loss training: 1.0355\n",
      "batch 1026/ 2342, loss training: 0.9192\n",
      "batch 1027/ 2342, loss training: 1.0096\n",
      "batch 1028/ 2342, loss training: 0.8503\n",
      "batch 1029/ 2342, loss training: 1.0610\n",
      "batch 1030/ 2342, loss training: 0.9599\n",
      "batch 1031/ 2342, loss training: 0.9382\n",
      "batch 1032/ 2342, loss training: 1.0778\n",
      "batch 1033/ 2342, loss training: 0.9548\n",
      "batch 1034/ 2342, loss training: 0.9083\n",
      "batch 1035/ 2342, loss training: 0.9238\n",
      "batch 1036/ 2342, loss training: 0.8910\n",
      "batch 1037/ 2342, loss training: 0.9919\n",
      "batch 1038/ 2342, loss training: 0.9671\n",
      "batch 1039/ 2342, loss training: 1.0064\n",
      "batch 1040/ 2342, loss training: 0.8163\n",
      "batch 1041/ 2342, loss training: 1.1157\n",
      "batch 1042/ 2342, loss training: 0.8815\n",
      "batch 1043/ 2342, loss training: 0.8165\n",
      "batch 1044/ 2342, loss training: 0.8465\n",
      "batch 1045/ 2342, loss training: 0.9967\n",
      "batch 1046/ 2342, loss training: 1.0136\n",
      "batch 1047/ 2342, loss training: 0.9769\n",
      "batch 1048/ 2342, loss training: 0.9660\n",
      "batch 1049/ 2342, loss training: 0.8751\n",
      "batch 1050/ 2342, loss training: 0.9687\n",
      "batch 1051/ 2342, loss training: 0.8893\n",
      "batch 1052/ 2342, loss training: 1.1800\n",
      "batch 1053/ 2342, loss training: 0.8833\n",
      "batch 1054/ 2342, loss training: 1.0522\n",
      "batch 1055/ 2342, loss training: 0.8612\n",
      "batch 1056/ 2342, loss training: 1.0082\n",
      "batch 1057/ 2342, loss training: 0.9663\n",
      "batch 1058/ 2342, loss training: 0.8523\n",
      "batch 1059/ 2342, loss training: 1.1197\n",
      "batch 1060/ 2342, loss training: 0.8954\n",
      "batch 1061/ 2342, loss training: 1.0482\n",
      "batch 1062/ 2342, loss training: 0.8019\n",
      "batch 1063/ 2342, loss training: 0.9912\n",
      "batch 1064/ 2342, loss training: 0.9854\n",
      "batch 1065/ 2342, loss training: 0.9323\n",
      "batch 1066/ 2342, loss training: 0.7814\n",
      "batch 1067/ 2342, loss training: 0.9273\n",
      "batch 1068/ 2342, loss training: 0.9504\n",
      "batch 1069/ 2342, loss training: 0.8060\n",
      "batch 1070/ 2342, loss training: 0.8243\n",
      "batch 1071/ 2342, loss training: 0.8589\n",
      "batch 1072/ 2342, loss training: 0.9695\n",
      "batch 1073/ 2342, loss training: 0.9910\n",
      "batch 1074/ 2342, loss training: 0.9992\n",
      "batch 1075/ 2342, loss training: 1.0494\n",
      "batch 1076/ 2342, loss training: 0.8094\n",
      "batch 1077/ 2342, loss training: 1.0207\n",
      "batch 1078/ 2342, loss training: 0.8122\n",
      "batch 1079/ 2342, loss training: 0.9565\n",
      "batch 1080/ 2342, loss training: 0.8342\n",
      "batch 1081/ 2342, loss training: 0.8225\n",
      "batch 1082/ 2342, loss training: 0.8705\n",
      "batch 1083/ 2342, loss training: 0.8690\n",
      "batch 1084/ 2342, loss training: 1.0534\n",
      "batch 1085/ 2342, loss training: 0.9029\n",
      "batch 1086/ 2342, loss training: 0.7745\n",
      "batch 1087/ 2342, loss training: 0.8857\n",
      "batch 1088/ 2342, loss training: 0.9220\n",
      "batch 1089/ 2342, loss training: 0.8762\n",
      "batch 1090/ 2342, loss training: 0.9262\n",
      "batch 1091/ 2342, loss training: 0.9487\n",
      "batch 1092/ 2342, loss training: 1.0090\n",
      "batch 1093/ 2342, loss training: 0.9706\n",
      "batch 1094/ 2342, loss training: 0.9707\n",
      "batch 1095/ 2342, loss training: 1.0451\n",
      "batch 1096/ 2342, loss training: 0.8558\n",
      "batch 1097/ 2342, loss training: 0.8795\n",
      "batch 1098/ 2342, loss training: 0.7860\n",
      "batch 1099/ 2342, loss training: 0.8853\n",
      "batch 1100/ 2342, loss training: 0.9235\n",
      "batch 1101/ 2342, loss training: 0.9424\n",
      "batch 1102/ 2342, loss training: 0.9578\n",
      "batch 1103/ 2342, loss training: 1.0577\n",
      "batch 1104/ 2342, loss training: 0.8434\n",
      "batch 1105/ 2342, loss training: 0.8868\n",
      "batch 1106/ 2342, loss training: 0.9124\n",
      "batch 1107/ 2342, loss training: 0.7454\n",
      "batch 1108/ 2342, loss training: 0.8047\n",
      "batch 1109/ 2342, loss training: 0.8270\n",
      "batch 1110/ 2342, loss training: 0.9265\n",
      "batch 1111/ 2342, loss training: 0.9941\n",
      "batch 1112/ 2342, loss training: 0.8937\n",
      "batch 1113/ 2342, loss training: 0.9842\n",
      "batch 1114/ 2342, loss training: 0.7433\n",
      "batch 1115/ 2342, loss training: 0.8191\n",
      "batch 1116/ 2342, loss training: 0.8366\n",
      "batch 1117/ 2342, loss training: 0.9606\n",
      "batch 1118/ 2342, loss training: 0.7706\n",
      "batch 1119/ 2342, loss training: 1.0277\n",
      "batch 1120/ 2342, loss training: 0.9971\n",
      "batch 1121/ 2342, loss training: 0.9076\n",
      "batch 1122/ 2342, loss training: 0.9237\n",
      "batch 1123/ 2342, loss training: 0.8954\n",
      "batch 1124/ 2342, loss training: 0.9377\n",
      "batch 1125/ 2342, loss training: 0.8882\n",
      "batch 1126/ 2342, loss training: 0.7307\n",
      "batch 1127/ 2342, loss training: 0.9706\n",
      "batch 1128/ 2342, loss training: 0.8777\n",
      "batch 1129/ 2342, loss training: 0.9334\n",
      "batch 1130/ 2342, loss training: 0.9667\n",
      "batch 1131/ 2342, loss training: 0.9439\n",
      "batch 1132/ 2342, loss training: 0.8403\n",
      "batch 1133/ 2342, loss training: 0.7733\n",
      "batch 1134/ 2342, loss training: 0.8613\n",
      "batch 1135/ 2342, loss training: 1.0486\n",
      "batch 1136/ 2342, loss training: 1.0183\n",
      "batch 1137/ 2342, loss training: 0.8453\n",
      "batch 1138/ 2342, loss training: 0.8463\n",
      "batch 1139/ 2342, loss training: 0.8906\n",
      "batch 1140/ 2342, loss training: 0.9049\n",
      "batch 1141/ 2342, loss training: 0.8909\n",
      "batch 1142/ 2342, loss training: 0.8405\n",
      "batch 1143/ 2342, loss training: 0.9886\n",
      "batch 1144/ 2342, loss training: 0.9869\n",
      "batch 1145/ 2342, loss training: 0.8770\n",
      "batch 1146/ 2342, loss training: 0.8748\n",
      "batch 1147/ 2342, loss training: 0.9392\n",
      "batch 1148/ 2342, loss training: 0.7688\n",
      "batch 1149/ 2342, loss training: 0.8314\n",
      "batch 1150/ 2342, loss training: 0.9925\n",
      "batch 1151/ 2342, loss training: 0.9267\n",
      "batch 1152/ 2342, loss training: 0.8138\n",
      "batch 1153/ 2342, loss training: 0.8947\n",
      "batch 1154/ 2342, loss training: 0.8131\n",
      "batch 1155/ 2342, loss training: 0.8759\n",
      "batch 1156/ 2342, loss training: 0.9554\n",
      "batch 1157/ 2342, loss training: 0.8952\n",
      "batch 1158/ 2342, loss training: 0.8385\n",
      "batch 1159/ 2342, loss training: 0.8635\n",
      "batch 1160/ 2342, loss training: 0.9267\n",
      "batch 1161/ 2342, loss training: 0.8172\n",
      "batch 1162/ 2342, loss training: 0.9224\n",
      "batch 1163/ 2342, loss training: 0.8153\n",
      "batch 1164/ 2342, loss training: 0.9706\n",
      "batch 1165/ 2342, loss training: 0.8735\n",
      "batch 1166/ 2342, loss training: 0.8769\n",
      "batch 1167/ 2342, loss training: 0.9597\n",
      "batch 1168/ 2342, loss training: 0.8597\n",
      "batch 1169/ 2342, loss training: 0.8049\n",
      "batch 1170/ 2342, loss training: 0.6882\n",
      "batch 1171/ 2342, loss training: 0.8458\n",
      "batch 1172/ 2342, loss training: 0.9425\n",
      "batch 1173/ 2342, loss training: 0.8583\n",
      "batch 1174/ 2342, loss training: 0.7565\n",
      "batch 1175/ 2342, loss training: 0.8887\n",
      "batch 1176/ 2342, loss training: 0.9016\n",
      "batch 1177/ 2342, loss training: 0.8031\n",
      "batch 1178/ 2342, loss training: 0.7123\n",
      "batch 1179/ 2342, loss training: 0.8113\n",
      "batch 1180/ 2342, loss training: 0.8231\n",
      "batch 1181/ 2342, loss training: 0.8383\n",
      "batch 1182/ 2342, loss training: 0.9549\n",
      "batch 1183/ 2342, loss training: 0.7243\n",
      "batch 1184/ 2342, loss training: 0.8391\n",
      "batch 1185/ 2342, loss training: 0.8756\n",
      "batch 1186/ 2342, loss training: 0.8141\n",
      "batch 1187/ 2342, loss training: 0.8054\n",
      "batch 1188/ 2342, loss training: 0.8991\n",
      "batch 1189/ 2342, loss training: 0.8808\n",
      "batch 1190/ 2342, loss training: 0.7371\n",
      "batch 1191/ 2342, loss training: 0.8529\n",
      "batch 1192/ 2342, loss training: 0.8806\n",
      "batch 1193/ 2342, loss training: 0.7504\n",
      "batch 1194/ 2342, loss training: 0.9780\n",
      "batch 1195/ 2342, loss training: 0.8718\n",
      "batch 1196/ 2342, loss training: 0.9459\n",
      "batch 1197/ 2342, loss training: 0.7893\n",
      "batch 1198/ 2342, loss training: 0.8113\n",
      "batch 1199/ 2342, loss training: 0.9554\n",
      "batch 1200/ 2342, loss training: 0.9811\n",
      "batch 1201/ 2342, loss training: 0.8245\n",
      "batch 1202/ 2342, loss training: 0.8237\n",
      "batch 1203/ 2342, loss training: 0.9067\n",
      "batch 1204/ 2342, loss training: 0.9457\n",
      "batch 1205/ 2342, loss training: 1.0284\n",
      "batch 1206/ 2342, loss training: 0.8068\n",
      "batch 1207/ 2342, loss training: 0.7798\n",
      "batch 1208/ 2342, loss training: 0.9823\n",
      "batch 1209/ 2342, loss training: 0.7956\n",
      "batch 1210/ 2342, loss training: 0.9179\n",
      "batch 1211/ 2342, loss training: 0.6890\n",
      "batch 1212/ 2342, loss training: 0.8938\n",
      "batch 1213/ 2342, loss training: 0.7879\n",
      "batch 1214/ 2342, loss training: 0.8540\n",
      "batch 1215/ 2342, loss training: 0.9212\n",
      "batch 1216/ 2342, loss training: 0.9468\n",
      "batch 1217/ 2342, loss training: 0.8563\n",
      "batch 1218/ 2342, loss training: 0.8263\n",
      "batch 1219/ 2342, loss training: 0.8937\n",
      "batch 1220/ 2342, loss training: 0.8049\n",
      "batch 1221/ 2342, loss training: 0.8624\n",
      "batch 1222/ 2342, loss training: 0.9261\n",
      "batch 1223/ 2342, loss training: 0.7650\n",
      "batch 1224/ 2342, loss training: 0.8627\n",
      "batch 1225/ 2342, loss training: 0.8600\n",
      "batch 1226/ 2342, loss training: 0.7031\n",
      "batch 1227/ 2342, loss training: 0.8304\n",
      "batch 1228/ 2342, loss training: 0.8510\n",
      "batch 1229/ 2342, loss training: 0.7159\n",
      "batch 1230/ 2342, loss training: 0.7205\n",
      "batch 1231/ 2342, loss training: 0.7688\n",
      "batch 1232/ 2342, loss training: 0.8053\n",
      "batch 1233/ 2342, loss training: 0.7226\n",
      "batch 1234/ 2342, loss training: 0.7849\n",
      "batch 1235/ 2342, loss training: 0.7533\n",
      "batch 1236/ 2342, loss training: 0.8046\n",
      "batch 1237/ 2342, loss training: 0.7708\n",
      "batch 1238/ 2342, loss training: 0.7607\n",
      "batch 1239/ 2342, loss training: 0.7616\n",
      "batch 1240/ 2342, loss training: 0.8036\n",
      "batch 1241/ 2342, loss training: 0.7808\n",
      "batch 1242/ 2342, loss training: 0.9083\n",
      "batch 1243/ 2342, loss training: 0.9367\n",
      "batch 1244/ 2342, loss training: 0.8974\n",
      "batch 1245/ 2342, loss training: 0.9177\n",
      "batch 1246/ 2342, loss training: 0.7865\n",
      "batch 1247/ 2342, loss training: 0.9695\n",
      "batch 1248/ 2342, loss training: 0.9513\n",
      "batch 1249/ 2342, loss training: 0.9535\n",
      "batch 1250/ 2342, loss training: 0.8546\n",
      "batch 1251/ 2342, loss training: 0.9704\n",
      "batch 1252/ 2342, loss training: 0.8311\n",
      "batch 1253/ 2342, loss training: 0.7496\n",
      "batch 1254/ 2342, loss training: 0.7811\n",
      "batch 1255/ 2342, loss training: 0.8650\n",
      "batch 1256/ 2342, loss training: 0.8801\n",
      "batch 1257/ 2342, loss training: 0.7643\n",
      "batch 1258/ 2342, loss training: 0.7095\n",
      "batch 1259/ 2342, loss training: 0.7332\n",
      "batch 1260/ 2342, loss training: 0.9523\n",
      "batch 1261/ 2342, loss training: 0.7198\n",
      "batch 1262/ 2342, loss training: 0.7409\n",
      "batch 1263/ 2342, loss training: 0.8032\n",
      "batch 1264/ 2342, loss training: 0.9714\n",
      "batch 1265/ 2342, loss training: 0.8113\n",
      "batch 1266/ 2342, loss training: 0.7970\n",
      "batch 1267/ 2342, loss training: 0.7639\n",
      "batch 1268/ 2342, loss training: 0.8086\n",
      "batch 1269/ 2342, loss training: 0.8519\n",
      "batch 1270/ 2342, loss training: 0.8417\n",
      "batch 1271/ 2342, loss training: 0.8389\n",
      "batch 1272/ 2342, loss training: 0.8845\n",
      "batch 1273/ 2342, loss training: 0.7720\n",
      "batch 1274/ 2342, loss training: 0.9099\n",
      "batch 1275/ 2342, loss training: 0.9719\n",
      "batch 1276/ 2342, loss training: 0.9057\n",
      "batch 1277/ 2342, loss training: 0.6654\n",
      "batch 1278/ 2342, loss training: 0.7903\n",
      "batch 1279/ 2342, loss training: 0.7656\n",
      "batch 1280/ 2342, loss training: 0.7318\n",
      "batch 1281/ 2342, loss training: 0.7690\n",
      "batch 1282/ 2342, loss training: 0.7135\n",
      "batch 1283/ 2342, loss training: 0.8550\n",
      "batch 1284/ 2342, loss training: 0.7950\n",
      "batch 1285/ 2342, loss training: 0.8745\n",
      "batch 1286/ 2342, loss training: 0.8233\n",
      "batch 1287/ 2342, loss training: 0.6632\n",
      "batch 1288/ 2342, loss training: 0.9015\n",
      "batch 1289/ 2342, loss training: 0.8862\n",
      "batch 1290/ 2342, loss training: 0.8131\n",
      "batch 1291/ 2342, loss training: 0.7375\n",
      "batch 1292/ 2342, loss training: 0.7397\n",
      "batch 1293/ 2342, loss training: 0.8089\n",
      "batch 1294/ 2342, loss training: 0.7170\n",
      "batch 1295/ 2342, loss training: 0.7941\n",
      "batch 1296/ 2342, loss training: 0.7490\n",
      "batch 1297/ 2342, loss training: 0.8186\n",
      "batch 1298/ 2342, loss training: 0.6981\n",
      "batch 1299/ 2342, loss training: 0.8684\n",
      "batch 1300/ 2342, loss training: 0.8932\n",
      "batch 1301/ 2342, loss training: 0.8345\n",
      "batch 1302/ 2342, loss training: 0.9238\n",
      "batch 1303/ 2342, loss training: 0.7478\n",
      "batch 1304/ 2342, loss training: 0.9379\n",
      "batch 1305/ 2342, loss training: 0.7904\n",
      "batch 1306/ 2342, loss training: 0.6844\n",
      "batch 1307/ 2342, loss training: 0.7610\n",
      "batch 1308/ 2342, loss training: 0.8098\n",
      "batch 1309/ 2342, loss training: 0.8695\n",
      "batch 1310/ 2342, loss training: 0.8264\n",
      "batch 1311/ 2342, loss training: 0.7756\n",
      "batch 1312/ 2342, loss training: 0.7418\n",
      "batch 1313/ 2342, loss training: 0.8148\n",
      "batch 1314/ 2342, loss training: 0.8590\n",
      "batch 1315/ 2342, loss training: 0.7884\n",
      "batch 1316/ 2342, loss training: 0.7689\n",
      "batch 1317/ 2342, loss training: 0.7882\n",
      "batch 1318/ 2342, loss training: 0.8503\n",
      "batch 1319/ 2342, loss training: 0.8066\n",
      "batch 1320/ 2342, loss training: 0.7660\n",
      "batch 1321/ 2342, loss training: 0.6995\n",
      "batch 1322/ 2342, loss training: 0.7902\n",
      "batch 1323/ 2342, loss training: 0.8449\n",
      "batch 1324/ 2342, loss training: 0.8697\n",
      "batch 1325/ 2342, loss training: 0.7050\n",
      "batch 1326/ 2342, loss training: 0.7512\n",
      "batch 1327/ 2342, loss training: 0.7264\n",
      "batch 1328/ 2342, loss training: 0.7589\n",
      "batch 1329/ 2342, loss training: 0.8152\n",
      "batch 1330/ 2342, loss training: 0.7094\n",
      "batch 1331/ 2342, loss training: 0.6683\n",
      "batch 1332/ 2342, loss training: 0.7706\n",
      "batch 1333/ 2342, loss training: 0.7520\n",
      "batch 1334/ 2342, loss training: 0.6550\n",
      "batch 1335/ 2342, loss training: 0.7469\n",
      "batch 1336/ 2342, loss training: 0.6830\n",
      "batch 1337/ 2342, loss training: 0.7705\n",
      "batch 1338/ 2342, loss training: 0.7583\n",
      "batch 1339/ 2342, loss training: 0.8962\n",
      "batch 1340/ 2342, loss training: 0.7495\n",
      "batch 1341/ 2342, loss training: 0.8712\n",
      "batch 1342/ 2342, loss training: 0.8326\n",
      "batch 1343/ 2342, loss training: 0.8102\n",
      "batch 1344/ 2342, loss training: 0.8348\n",
      "batch 1345/ 2342, loss training: 0.6442\n",
      "batch 1346/ 2342, loss training: 0.7092\n",
      "batch 1347/ 2342, loss training: 0.7010\n",
      "batch 1348/ 2342, loss training: 0.8876\n",
      "batch 1349/ 2342, loss training: 0.8762\n",
      "batch 1350/ 2342, loss training: 0.8317\n",
      "batch 1351/ 2342, loss training: 0.7492\n",
      "batch 1352/ 2342, loss training: 0.7108\n",
      "batch 1353/ 2342, loss training: 0.7395\n",
      "batch 1354/ 2342, loss training: 0.7423\n",
      "batch 1355/ 2342, loss training: 0.7798\n",
      "batch 1356/ 2342, loss training: 0.8341\n",
      "batch 1357/ 2342, loss training: 0.7949\n",
      "batch 1358/ 2342, loss training: 0.8454\n",
      "batch 1359/ 2342, loss training: 0.7960\n",
      "batch 1360/ 2342, loss training: 0.8122\n",
      "batch 1361/ 2342, loss training: 0.8738\n",
      "batch 1362/ 2342, loss training: 0.7234\n",
      "batch 1363/ 2342, loss training: 0.7880\n",
      "batch 1364/ 2342, loss training: 0.7931\n",
      "batch 1365/ 2342, loss training: 0.8049\n",
      "batch 1366/ 2342, loss training: 0.6047\n",
      "batch 1367/ 2342, loss training: 0.7592\n",
      "batch 1368/ 2342, loss training: 0.8430\n",
      "batch 1369/ 2342, loss training: 0.8113\n",
      "batch 1370/ 2342, loss training: 0.8270\n",
      "batch 1371/ 2342, loss training: 0.6563\n",
      "batch 1372/ 2342, loss training: 0.7808\n",
      "batch 1373/ 2342, loss training: 0.7141\n",
      "batch 1374/ 2342, loss training: 0.8514\n",
      "batch 1375/ 2342, loss training: 0.7094\n",
      "batch 1376/ 2342, loss training: 0.8711\n",
      "batch 1377/ 2342, loss training: 0.7181\n",
      "batch 1378/ 2342, loss training: 0.8013\n",
      "batch 1379/ 2342, loss training: 0.8244\n",
      "batch 1380/ 2342, loss training: 0.7879\n",
      "batch 1381/ 2342, loss training: 0.8508\n",
      "batch 1382/ 2342, loss training: 0.7727\n",
      "batch 1383/ 2342, loss training: 0.7827\n",
      "batch 1384/ 2342, loss training: 0.7512\n",
      "batch 1385/ 2342, loss training: 0.6263\n",
      "batch 1386/ 2342, loss training: 0.7420\n",
      "batch 1387/ 2342, loss training: 0.8034\n",
      "batch 1388/ 2342, loss training: 0.8947\n",
      "batch 1389/ 2342, loss training: 0.8932\n",
      "batch 1390/ 2342, loss training: 0.8131\n",
      "batch 1391/ 2342, loss training: 0.7926\n",
      "batch 1392/ 2342, loss training: 0.6390\n",
      "batch 1393/ 2342, loss training: 0.7127\n",
      "batch 1394/ 2342, loss training: 0.8608\n",
      "batch 1395/ 2342, loss training: 0.7810\n",
      "batch 1396/ 2342, loss training: 0.6509\n",
      "batch 1397/ 2342, loss training: 0.8335\n",
      "batch 1398/ 2342, loss training: 0.7460\n",
      "batch 1399/ 2342, loss training: 0.8447\n",
      "batch 1400/ 2342, loss training: 0.8979\n",
      "batch 1401/ 2342, loss training: 0.6392\n",
      "batch 1402/ 2342, loss training: 0.7931\n",
      "batch 1403/ 2342, loss training: 0.9580\n",
      "batch 1404/ 2342, loss training: 0.7963\n",
      "batch 1405/ 2342, loss training: 0.6958\n",
      "batch 1406/ 2342, loss training: 0.8201\n",
      "batch 1407/ 2342, loss training: 0.8966\n",
      "batch 1408/ 2342, loss training: 0.8060\n",
      "batch 1409/ 2342, loss training: 0.7662\n",
      "batch 1410/ 2342, loss training: 0.8037\n",
      "batch 1411/ 2342, loss training: 0.9772\n",
      "batch 1412/ 2342, loss training: 0.8124\n",
      "batch 1413/ 2342, loss training: 0.7344\n",
      "batch 1414/ 2342, loss training: 0.8817\n",
      "batch 1415/ 2342, loss training: 0.8622\n",
      "batch 1416/ 2342, loss training: 0.7649\n",
      "batch 1417/ 2342, loss training: 0.8198\n",
      "batch 1418/ 2342, loss training: 0.9331\n",
      "batch 1419/ 2342, loss training: 0.7755\n",
      "batch 1420/ 2342, loss training: 0.9008\n",
      "batch 1421/ 2342, loss training: 0.7563\n",
      "batch 1422/ 2342, loss training: 0.7563\n",
      "batch 1423/ 2342, loss training: 0.6850\n",
      "batch 1424/ 2342, loss training: 0.6751\n",
      "batch 1425/ 2342, loss training: 0.7804\n",
      "batch 1426/ 2342, loss training: 0.8393\n",
      "batch 1427/ 2342, loss training: 0.7789\n",
      "batch 1428/ 2342, loss training: 0.7509\n",
      "batch 1429/ 2342, loss training: 0.7860\n",
      "batch 1430/ 2342, loss training: 0.9235\n",
      "batch 1431/ 2342, loss training: 0.7512\n",
      "batch 1432/ 2342, loss training: 0.7912\n",
      "batch 1433/ 2342, loss training: 0.7028\n",
      "batch 1434/ 2342, loss training: 0.8200\n",
      "batch 1435/ 2342, loss training: 0.7226\n",
      "batch 1436/ 2342, loss training: 0.6624\n",
      "batch 1437/ 2342, loss training: 0.8756\n",
      "batch 1438/ 2342, loss training: 0.7872\n",
      "batch 1439/ 2342, loss training: 0.7722\n",
      "batch 1440/ 2342, loss training: 0.7697\n",
      "batch 1441/ 2342, loss training: 0.8949\n",
      "batch 1442/ 2342, loss training: 0.7247\n",
      "batch 1443/ 2342, loss training: 0.6404\n",
      "batch 1444/ 2342, loss training: 0.7825\n",
      "batch 1445/ 2342, loss training: 0.7806\n",
      "batch 1446/ 2342, loss training: 0.7810\n",
      "batch 1447/ 2342, loss training: 0.8630\n",
      "batch 1448/ 2342, loss training: 0.7158\n",
      "batch 1449/ 2342, loss training: 0.7851\n",
      "batch 1450/ 2342, loss training: 0.6824\n",
      "batch 1451/ 2342, loss training: 0.8001\n",
      "batch 1452/ 2342, loss training: 0.8248\n",
      "batch 1453/ 2342, loss training: 0.8328\n",
      "batch 1454/ 2342, loss training: 0.6321\n",
      "batch 1455/ 2342, loss training: 0.6569\n",
      "batch 1456/ 2342, loss training: 0.7367\n",
      "batch 1457/ 2342, loss training: 0.7668\n",
      "batch 1458/ 2342, loss training: 0.8089\n",
      "batch 1459/ 2342, loss training: 0.7204\n",
      "batch 1460/ 2342, loss training: 0.6979\n",
      "batch 1461/ 2342, loss training: 0.7661\n",
      "batch 1462/ 2342, loss training: 0.7534\n",
      "batch 1463/ 2342, loss training: 0.8306\n",
      "batch 1464/ 2342, loss training: 0.8038\n",
      "batch 1465/ 2342, loss training: 0.7500\n",
      "batch 1466/ 2342, loss training: 0.7624\n",
      "batch 1467/ 2342, loss training: 0.6990\n",
      "batch 1468/ 2342, loss training: 0.7361\n",
      "batch 1469/ 2342, loss training: 0.6643\n",
      "batch 1470/ 2342, loss training: 0.6802\n",
      "batch 1471/ 2342, loss training: 0.7861\n",
      "batch 1472/ 2342, loss training: 0.7675\n",
      "batch 1473/ 2342, loss training: 0.7231\n",
      "batch 1474/ 2342, loss training: 0.7044\n",
      "batch 1475/ 2342, loss training: 0.6538\n",
      "batch 1476/ 2342, loss training: 0.6627\n",
      "batch 1477/ 2342, loss training: 0.6545\n",
      "batch 1478/ 2342, loss training: 0.8132\n",
      "batch 1479/ 2342, loss training: 0.7246\n",
      "batch 1480/ 2342, loss training: 0.7963\n",
      "batch 1481/ 2342, loss training: 0.6876\n",
      "batch 1482/ 2342, loss training: 0.6681\n",
      "batch 1483/ 2342, loss training: 0.8342\n",
      "batch 1484/ 2342, loss training: 0.6400\n",
      "batch 1485/ 2342, loss training: 0.7170\n",
      "batch 1486/ 2342, loss training: 0.6914\n",
      "batch 1487/ 2342, loss training: 0.7298\n",
      "batch 1488/ 2342, loss training: 0.8027\n",
      "batch 1489/ 2342, loss training: 0.7382\n",
      "batch 1490/ 2342, loss training: 0.6991\n",
      "batch 1491/ 2342, loss training: 0.6625\n",
      "batch 1492/ 2342, loss training: 0.6684\n",
      "batch 1493/ 2342, loss training: 0.7527\n",
      "batch 1494/ 2342, loss training: 0.5713\n",
      "batch 1495/ 2342, loss training: 0.7471\n",
      "batch 1496/ 2342, loss training: 0.6730\n",
      "batch 1497/ 2342, loss training: 0.6763\n",
      "batch 1498/ 2342, loss training: 0.7206\n",
      "batch 1499/ 2342, loss training: 0.6498\n",
      "batch 1500/ 2342, loss training: 0.7372\n",
      "batch 1501/ 2342, loss training: 0.8152\n",
      "batch 1502/ 2342, loss training: 0.7134\n",
      "batch 1503/ 2342, loss training: 0.7319\n",
      "batch 1504/ 2342, loss training: 0.7826\n",
      "batch 1505/ 2342, loss training: 0.7236\n",
      "batch 1506/ 2342, loss training: 0.7164\n",
      "batch 1507/ 2342, loss training: 0.6557\n",
      "batch 1508/ 2342, loss training: 0.7580\n",
      "batch 1509/ 2342, loss training: 0.7040\n",
      "batch 1510/ 2342, loss training: 0.8086\n",
      "batch 1511/ 2342, loss training: 0.7062\n",
      "batch 1512/ 2342, loss training: 0.7827\n",
      "batch 1513/ 2342, loss training: 0.7303\n",
      "batch 1514/ 2342, loss training: 0.7727\n",
      "batch 1515/ 2342, loss training: 0.7977\n",
      "batch 1516/ 2342, loss training: 0.6699\n",
      "batch 1517/ 2342, loss training: 0.7248\n",
      "batch 1518/ 2342, loss training: 0.7390\n",
      "batch 1519/ 2342, loss training: 0.7389\n",
      "batch 1520/ 2342, loss training: 0.8344\n",
      "batch 1521/ 2342, loss training: 0.7205\n",
      "batch 1522/ 2342, loss training: 0.6421\n",
      "batch 1523/ 2342, loss training: 0.6526\n",
      "batch 1524/ 2342, loss training: 0.7969\n",
      "batch 1525/ 2342, loss training: 0.8347\n",
      "batch 1526/ 2342, loss training: 0.6845\n",
      "batch 1527/ 2342, loss training: 0.5990\n",
      "batch 1528/ 2342, loss training: 0.8342\n",
      "batch 1529/ 2342, loss training: 0.6728\n",
      "batch 1530/ 2342, loss training: 0.8127\n",
      "batch 1531/ 2342, loss training: 0.7124\n",
      "batch 1532/ 2342, loss training: 0.6952\n",
      "batch 1533/ 2342, loss training: 0.9035\n",
      "batch 1534/ 2342, loss training: 0.7325\n",
      "batch 1535/ 2342, loss training: 0.6950\n",
      "batch 1536/ 2342, loss training: 0.7996\n",
      "batch 1537/ 2342, loss training: 0.7295\n",
      "batch 1538/ 2342, loss training: 0.6327\n",
      "batch 1539/ 2342, loss training: 0.8484\n",
      "batch 1540/ 2342, loss training: 0.7976\n",
      "batch 1541/ 2342, loss training: 0.5768\n",
      "batch 1542/ 2342, loss training: 0.6856\n",
      "batch 1543/ 2342, loss training: 0.7612\n",
      "batch 1544/ 2342, loss training: 0.7894\n",
      "batch 1545/ 2342, loss training: 0.7430\n",
      "batch 1546/ 2342, loss training: 0.6393\n",
      "batch 1547/ 2342, loss training: 0.6191\n",
      "batch 1548/ 2342, loss training: 0.6659\n",
      "batch 1549/ 2342, loss training: 0.6934\n",
      "batch 1550/ 2342, loss training: 0.7651\n",
      "batch 1551/ 2342, loss training: 0.7476\n",
      "batch 1552/ 2342, loss training: 0.6054\n",
      "batch 1553/ 2342, loss training: 0.7314\n",
      "batch 1554/ 2342, loss training: 0.6358\n",
      "batch 1555/ 2342, loss training: 0.7962\n",
      "batch 1556/ 2342, loss training: 0.7082\n",
      "batch 1557/ 2342, loss training: 0.5624\n",
      "batch 1558/ 2342, loss training: 0.6889\n",
      "batch 1559/ 2342, loss training: 0.6546\n",
      "batch 1560/ 2342, loss training: 0.7149\n",
      "batch 1561/ 2342, loss training: 0.7567\n",
      "batch 1562/ 2342, loss training: 0.8127\n",
      "batch 1563/ 2342, loss training: 0.6030\n",
      "batch 1564/ 2342, loss training: 0.7585\n",
      "batch 1565/ 2342, loss training: 0.6791\n",
      "batch 1566/ 2342, loss training: 0.6747\n",
      "batch 1567/ 2342, loss training: 0.7353\n",
      "batch 1568/ 2342, loss training: 0.6909\n",
      "batch 1569/ 2342, loss training: 0.6590\n",
      "batch 1570/ 2342, loss training: 0.6272\n",
      "batch 1571/ 2342, loss training: 0.7455\n",
      "batch 1572/ 2342, loss training: 0.6952\n",
      "batch 1573/ 2342, loss training: 0.7750\n",
      "batch 1574/ 2342, loss training: 0.6820\n",
      "batch 1575/ 2342, loss training: 0.7327\n",
      "batch 1576/ 2342, loss training: 0.7175\n",
      "batch 1577/ 2342, loss training: 0.7436\n",
      "batch 1578/ 2342, loss training: 0.6459\n",
      "batch 1579/ 2342, loss training: 0.5758\n",
      "batch 1580/ 2342, loss training: 0.6512\n",
      "batch 1581/ 2342, loss training: 0.7399\n",
      "batch 1582/ 2342, loss training: 0.7275\n",
      "batch 1583/ 2342, loss training: 0.7911\n",
      "batch 1584/ 2342, loss training: 0.8327\n",
      "batch 1585/ 2342, loss training: 0.6872\n",
      "batch 1586/ 2342, loss training: 0.7327\n",
      "batch 1587/ 2342, loss training: 0.6981\n",
      "batch 1588/ 2342, loss training: 0.5863\n",
      "batch 1589/ 2342, loss training: 0.6606\n",
      "batch 1590/ 2342, loss training: 0.6776\n",
      "batch 1591/ 2342, loss training: 0.7533\n",
      "batch 1592/ 2342, loss training: 0.8200\n",
      "batch 1593/ 2342, loss training: 0.6597\n",
      "batch 1594/ 2342, loss training: 0.6400\n",
      "batch 1595/ 2342, loss training: 0.5894\n",
      "batch 1596/ 2342, loss training: 0.6806\n",
      "batch 1597/ 2342, loss training: 0.6337\n",
      "batch 1598/ 2342, loss training: 0.6024\n",
      "batch 1599/ 2342, loss training: 0.6783\n",
      "batch 1600/ 2342, loss training: 0.7083\n",
      "batch 1601/ 2342, loss training: 0.7058\n",
      "batch 1602/ 2342, loss training: 0.7433\n",
      "batch 1603/ 2342, loss training: 0.7131\n",
      "batch 1604/ 2342, loss training: 0.7000\n",
      "batch 1605/ 2342, loss training: 0.6547\n",
      "batch 1606/ 2342, loss training: 0.7591\n",
      "batch 1607/ 2342, loss training: 0.7301\n",
      "batch 1608/ 2342, loss training: 0.6663\n",
      "batch 1609/ 2342, loss training: 0.7777\n",
      "batch 1610/ 2342, loss training: 0.7185\n",
      "batch 1611/ 2342, loss training: 0.6661\n",
      "batch 1612/ 2342, loss training: 0.8689\n",
      "batch 1613/ 2342, loss training: 0.6769\n",
      "batch 1614/ 2342, loss training: 0.6245\n",
      "batch 1615/ 2342, loss training: 0.7583\n",
      "batch 1616/ 2342, loss training: 0.7203\n",
      "batch 1617/ 2342, loss training: 0.7940\n",
      "batch 1618/ 2342, loss training: 0.6559\n",
      "batch 1619/ 2342, loss training: 0.6839\n",
      "batch 1620/ 2342, loss training: 0.6613\n",
      "batch 1621/ 2342, loss training: 0.7314\n",
      "batch 1622/ 2342, loss training: 0.6220\n",
      "batch 1623/ 2342, loss training: 0.6817\n",
      "batch 1624/ 2342, loss training: 0.6935\n",
      "batch 1625/ 2342, loss training: 0.7966\n",
      "batch 1626/ 2342, loss training: 0.6905\n",
      "batch 1627/ 2342, loss training: 0.7018\n",
      "batch 1628/ 2342, loss training: 0.7985\n",
      "batch 1629/ 2342, loss training: 0.7777\n",
      "batch 1630/ 2342, loss training: 0.6533\n",
      "batch 1631/ 2342, loss training: 0.7657\n",
      "batch 1632/ 2342, loss training: 0.5524\n",
      "batch 1633/ 2342, loss training: 0.7661\n",
      "batch 1634/ 2342, loss training: 0.6123\n",
      "batch 1635/ 2342, loss training: 0.6460\n",
      "batch 1636/ 2342, loss training: 0.7312\n",
      "batch 1637/ 2342, loss training: 0.6598\n",
      "batch 1638/ 2342, loss training: 0.7643\n",
      "batch 1639/ 2342, loss training: 0.7534\n",
      "batch 1640/ 2342, loss training: 0.6971\n",
      "batch 1641/ 2342, loss training: 0.6836\n",
      "batch 1642/ 2342, loss training: 0.8051\n",
      "batch 1643/ 2342, loss training: 0.5341\n",
      "batch 1644/ 2342, loss training: 0.8624\n",
      "batch 1645/ 2342, loss training: 0.7217\n",
      "batch 1646/ 2342, loss training: 0.6571\n",
      "batch 1647/ 2342, loss training: 0.7361\n",
      "batch 1648/ 2342, loss training: 0.6242\n",
      "batch 1649/ 2342, loss training: 0.6174\n",
      "batch 1650/ 2342, loss training: 0.6289\n",
      "batch 1651/ 2342, loss training: 0.7308\n",
      "batch 1652/ 2342, loss training: 0.8339\n",
      "batch 1653/ 2342, loss training: 0.7251\n",
      "batch 1654/ 2342, loss training: 0.6667\n",
      "batch 1655/ 2342, loss training: 0.7228\n",
      "batch 1656/ 2342, loss training: 0.5789\n",
      "batch 1657/ 2342, loss training: 0.5987\n",
      "batch 1658/ 2342, loss training: 0.5673\n",
      "batch 1659/ 2342, loss training: 0.6333\n",
      "batch 1660/ 2342, loss training: 0.6110\n",
      "batch 1661/ 2342, loss training: 0.7347\n",
      "batch 1662/ 2342, loss training: 0.6843\n",
      "batch 1663/ 2342, loss training: 0.5701\n",
      "batch 1664/ 2342, loss training: 0.6715\n",
      "batch 1665/ 2342, loss training: 0.7235\n",
      "batch 1666/ 2342, loss training: 0.5846\n",
      "batch 1667/ 2342, loss training: 0.7205\n",
      "batch 1668/ 2342, loss training: 0.6256\n",
      "batch 1669/ 2342, loss training: 0.6473\n",
      "batch 1670/ 2342, loss training: 0.7248\n",
      "batch 1671/ 2342, loss training: 0.6462\n",
      "batch 1672/ 2342, loss training: 0.6705\n",
      "batch 1673/ 2342, loss training: 0.6362\n",
      "batch 1674/ 2342, loss training: 0.7601\n",
      "batch 1675/ 2342, loss training: 0.6980\n",
      "batch 1676/ 2342, loss training: 0.7983\n",
      "batch 1677/ 2342, loss training: 0.8745\n",
      "batch 1678/ 2342, loss training: 0.6755\n",
      "batch 1679/ 2342, loss training: 0.6622\n",
      "batch 1680/ 2342, loss training: 0.7448\n",
      "batch 1681/ 2342, loss training: 0.5368\n",
      "batch 1682/ 2342, loss training: 0.7078\n",
      "batch 1683/ 2342, loss training: 0.7214\n",
      "batch 1684/ 2342, loss training: 0.5662\n",
      "batch 1685/ 2342, loss training: 0.7366\n",
      "batch 1686/ 2342, loss training: 0.5806\n",
      "batch 1687/ 2342, loss training: 0.7119\n",
      "batch 1688/ 2342, loss training: 0.6700\n",
      "batch 1689/ 2342, loss training: 0.7605\n",
      "batch 1690/ 2342, loss training: 0.6443\n",
      "batch 1691/ 2342, loss training: 0.6933\n",
      "batch 1692/ 2342, loss training: 0.6895\n",
      "batch 1693/ 2342, loss training: 0.6207\n",
      "batch 1694/ 2342, loss training: 0.6742\n",
      "batch 1695/ 2342, loss training: 0.6117\n",
      "batch 1696/ 2342, loss training: 0.8025\n",
      "batch 1697/ 2342, loss training: 0.7292\n",
      "batch 1698/ 2342, loss training: 0.6757\n",
      "batch 1699/ 2342, loss training: 0.6494\n",
      "batch 1700/ 2342, loss training: 0.7596\n",
      "batch 1701/ 2342, loss training: 0.6394\n",
      "batch 1702/ 2342, loss training: 0.7382\n",
      "batch 1703/ 2342, loss training: 0.7399\n",
      "batch 1704/ 2342, loss training: 0.6386\n",
      "batch 1705/ 2342, loss training: 0.7584\n",
      "batch 1706/ 2342, loss training: 0.5367\n",
      "batch 1707/ 2342, loss training: 0.4985\n",
      "batch 1708/ 2342, loss training: 0.5955\n",
      "batch 1709/ 2342, loss training: 0.7901\n",
      "batch 1710/ 2342, loss training: 0.7951\n",
      "batch 1711/ 2342, loss training: 0.6433\n",
      "batch 1712/ 2342, loss training: 0.7978\n",
      "batch 1713/ 2342, loss training: 0.6802\n",
      "batch 1714/ 2342, loss training: 0.6907\n",
      "batch 1715/ 2342, loss training: 0.7200\n",
      "batch 1716/ 2342, loss training: 0.6631\n",
      "batch 1717/ 2342, loss training: 0.7135\n",
      "batch 1718/ 2342, loss training: 0.5797\n",
      "batch 1719/ 2342, loss training: 0.6763\n",
      "batch 1720/ 2342, loss training: 0.6751\n",
      "batch 1721/ 2342, loss training: 0.6206\n",
      "batch 1722/ 2342, loss training: 0.5699\n",
      "batch 1723/ 2342, loss training: 0.5770\n",
      "batch 1724/ 2342, loss training: 0.7400\n",
      "batch 1725/ 2342, loss training: 0.6671\n",
      "batch 1726/ 2342, loss training: 0.6614\n",
      "batch 1727/ 2342, loss training: 0.7052\n",
      "batch 1728/ 2342, loss training: 0.7821\n",
      "batch 1729/ 2342, loss training: 0.7069\n",
      "batch 1730/ 2342, loss training: 0.6538\n",
      "batch 1731/ 2342, loss training: 0.7479\n",
      "batch 1732/ 2342, loss training: 0.6096\n",
      "batch 1733/ 2342, loss training: 0.6906\n",
      "batch 1734/ 2342, loss training: 0.6541\n",
      "batch 1735/ 2342, loss training: 0.7205\n",
      "batch 1736/ 2342, loss training: 0.5667\n",
      "batch 1737/ 2342, loss training: 0.6143\n",
      "batch 1738/ 2342, loss training: 0.7637\n",
      "batch 1739/ 2342, loss training: 0.7227\n",
      "batch 1740/ 2342, loss training: 0.6916\n",
      "batch 1741/ 2342, loss training: 0.7129\n",
      "batch 1742/ 2342, loss training: 0.5914\n",
      "batch 1743/ 2342, loss training: 0.6637\n",
      "batch 1744/ 2342, loss training: 0.7503\n",
      "batch 1745/ 2342, loss training: 0.5933\n",
      "batch 1746/ 2342, loss training: 0.6497\n",
      "batch 1747/ 2342, loss training: 0.6435\n",
      "batch 1748/ 2342, loss training: 0.6665\n",
      "batch 1749/ 2342, loss training: 0.5825\n",
      "batch 1750/ 2342, loss training: 0.7220\n",
      "batch 1751/ 2342, loss training: 0.7172\n",
      "batch 1752/ 2342, loss training: 0.6119\n",
      "batch 1753/ 2342, loss training: 0.7090\n",
      "batch 1754/ 2342, loss training: 0.5762\n",
      "batch 1755/ 2342, loss training: 0.6354\n",
      "batch 1756/ 2342, loss training: 0.5364\n",
      "batch 1757/ 2342, loss training: 0.6115\n",
      "batch 1758/ 2342, loss training: 0.7244\n",
      "batch 1759/ 2342, loss training: 0.6449\n",
      "batch 1760/ 2342, loss training: 0.5999\n",
      "batch 1761/ 2342, loss training: 0.5102\n",
      "batch 1762/ 2342, loss training: 0.7415\n",
      "batch 1763/ 2342, loss training: 0.6734\n",
      "batch 1764/ 2342, loss training: 0.6822\n",
      "batch 1765/ 2342, loss training: 0.7199\n",
      "batch 1766/ 2342, loss training: 0.6699\n",
      "batch 1767/ 2342, loss training: 0.6715\n",
      "batch 1768/ 2342, loss training: 0.6423\n",
      "batch 1769/ 2342, loss training: 0.5695\n",
      "batch 1770/ 2342, loss training: 0.8049\n",
      "batch 1771/ 2342, loss training: 0.5208\n",
      "batch 1772/ 2342, loss training: 0.7017\n",
      "batch 1773/ 2342, loss training: 0.7373\n",
      "batch 1774/ 2342, loss training: 0.6842\n",
      "batch 1775/ 2342, loss training: 0.5776\n",
      "batch 1776/ 2342, loss training: 0.7123\n",
      "batch 1777/ 2342, loss training: 0.7370\n",
      "batch 1778/ 2342, loss training: 0.5967\n",
      "batch 1779/ 2342, loss training: 0.6212\n",
      "batch 1780/ 2342, loss training: 0.5909\n",
      "batch 1781/ 2342, loss training: 0.7118\n",
      "batch 1782/ 2342, loss training: 0.7297\n",
      "batch 1783/ 2342, loss training: 0.6171\n",
      "batch 1784/ 2342, loss training: 0.6924\n",
      "batch 1785/ 2342, loss training: 0.6682\n",
      "batch 1786/ 2342, loss training: 0.5907\n",
      "batch 1787/ 2342, loss training: 0.5404\n",
      "batch 1788/ 2342, loss training: 0.7437\n",
      "batch 1789/ 2342, loss training: 0.5363\n",
      "batch 1790/ 2342, loss training: 0.6001\n",
      "batch 1791/ 2342, loss training: 0.8629\n",
      "batch 1792/ 2342, loss training: 0.6183\n",
      "batch 1793/ 2342, loss training: 0.5373\n",
      "batch 1794/ 2342, loss training: 0.6010\n",
      "batch 1795/ 2342, loss training: 0.6777\n",
      "batch 1796/ 2342, loss training: 0.6495\n",
      "batch 1797/ 2342, loss training: 0.6770\n",
      "batch 1798/ 2342, loss training: 0.6210\n",
      "batch 1799/ 2342, loss training: 0.5961\n",
      "batch 1800/ 2342, loss training: 0.6615\n",
      "batch 1801/ 2342, loss training: 0.5996\n",
      "batch 1802/ 2342, loss training: 0.5703\n",
      "batch 1803/ 2342, loss training: 0.6067\n",
      "batch 1804/ 2342, loss training: 0.6143\n",
      "batch 1805/ 2342, loss training: 0.6475\n",
      "batch 1806/ 2342, loss training: 0.6455\n",
      "batch 1807/ 2342, loss training: 0.5965\n",
      "batch 1808/ 2342, loss training: 0.6536\n",
      "batch 1809/ 2342, loss training: 0.6492\n",
      "batch 1810/ 2342, loss training: 0.6924\n",
      "batch 1811/ 2342, loss training: 0.5657\n",
      "batch 1812/ 2342, loss training: 0.7498\n",
      "batch 1813/ 2342, loss training: 0.7035\n",
      "batch 1814/ 2342, loss training: 0.6850\n",
      "batch 1815/ 2342, loss training: 0.6394\n",
      "batch 1816/ 2342, loss training: 0.6798\n",
      "batch 1817/ 2342, loss training: 0.7269\n",
      "batch 1818/ 2342, loss training: 0.6398\n",
      "batch 1819/ 2342, loss training: 0.5714\n",
      "batch 1820/ 2342, loss training: 0.6198\n",
      "batch 1821/ 2342, loss training: 0.6517\n",
      "batch 1822/ 2342, loss training: 0.6652\n",
      "batch 1823/ 2342, loss training: 0.7309\n",
      "batch 1824/ 2342, loss training: 0.6006\n",
      "batch 1825/ 2342, loss training: 0.5731\n",
      "batch 1826/ 2342, loss training: 0.7425\n",
      "batch 1827/ 2342, loss training: 0.5561\n",
      "batch 1828/ 2342, loss training: 0.6844\n",
      "batch 1829/ 2342, loss training: 0.8838\n",
      "batch 1830/ 2342, loss training: 0.5687\n",
      "batch 1831/ 2342, loss training: 0.5847\n",
      "batch 1832/ 2342, loss training: 0.5446\n",
      "batch 1833/ 2342, loss training: 0.6800\n",
      "batch 1834/ 2342, loss training: 0.6240\n",
      "batch 1835/ 2342, loss training: 0.6812\n",
      "batch 1836/ 2342, loss training: 0.6493\n",
      "batch 1837/ 2342, loss training: 0.6250\n",
      "batch 1838/ 2342, loss training: 0.6461\n",
      "batch 1839/ 2342, loss training: 0.5387\n",
      "batch 1840/ 2342, loss training: 0.6658\n",
      "batch 1841/ 2342, loss training: 0.6958\n",
      "batch 1842/ 2342, loss training: 0.6151\n",
      "batch 1843/ 2342, loss training: 0.7934\n",
      "batch 1844/ 2342, loss training: 0.5904\n",
      "batch 1845/ 2342, loss training: 0.6600\n",
      "batch 1846/ 2342, loss training: 0.6391\n",
      "batch 1847/ 2342, loss training: 0.6410\n",
      "batch 1848/ 2342, loss training: 0.6478\n",
      "batch 1849/ 2342, loss training: 0.6605\n",
      "batch 1850/ 2342, loss training: 0.6489\n",
      "batch 1851/ 2342, loss training: 0.7177\n",
      "batch 1852/ 2342, loss training: 0.5417\n",
      "batch 1853/ 2342, loss training: 0.6020\n",
      "batch 1854/ 2342, loss training: 0.6830\n",
      "batch 1855/ 2342, loss training: 0.6511\n",
      "batch 1856/ 2342, loss training: 0.7109\n",
      "batch 1857/ 2342, loss training: 0.6795\n",
      "batch 1858/ 2342, loss training: 0.6730\n",
      "batch 1859/ 2342, loss training: 0.6794\n",
      "batch 1860/ 2342, loss training: 0.5602\n",
      "batch 1861/ 2342, loss training: 0.6289\n",
      "batch 1862/ 2342, loss training: 0.6157\n",
      "batch 1863/ 2342, loss training: 0.5914\n",
      "batch 1864/ 2342, loss training: 0.6013\n",
      "batch 1865/ 2342, loss training: 0.6461\n",
      "batch 1866/ 2342, loss training: 0.6142\n",
      "batch 1867/ 2342, loss training: 0.6221\n",
      "batch 1868/ 2342, loss training: 0.6553\n",
      "batch 1869/ 2342, loss training: 0.5643\n",
      "batch 1870/ 2342, loss training: 0.6270\n",
      "batch 1871/ 2342, loss training: 0.7292\n",
      "batch 1872/ 2342, loss training: 0.7070\n",
      "batch 1873/ 2342, loss training: 0.6545\n",
      "batch 1874/ 2342, loss training: 0.5993\n",
      "batch 1875/ 2342, loss training: 0.6386\n",
      "batch 1876/ 2342, loss training: 0.5538\n",
      "batch 1877/ 2342, loss training: 0.6645\n",
      "batch 1878/ 2342, loss training: 0.6587\n",
      "batch 1879/ 2342, loss training: 0.7349\n",
      "batch 1880/ 2342, loss training: 0.7262\n",
      "batch 1881/ 2342, loss training: 0.6885\n",
      "batch 1882/ 2342, loss training: 0.5773\n",
      "batch 1883/ 2342, loss training: 0.7466\n",
      "batch 1884/ 2342, loss training: 0.5739\n",
      "batch 1885/ 2342, loss training: 0.5893\n",
      "batch 1886/ 2342, loss training: 0.6692\n",
      "batch 1887/ 2342, loss training: 0.6798\n",
      "batch 1888/ 2342, loss training: 0.7277\n",
      "batch 1889/ 2342, loss training: 0.5401\n",
      "batch 1890/ 2342, loss training: 0.6658\n",
      "batch 1891/ 2342, loss training: 0.6917\n",
      "batch 1892/ 2342, loss training: 0.6654\n",
      "batch 1893/ 2342, loss training: 0.7981\n",
      "batch 1894/ 2342, loss training: 0.7225\n",
      "batch 1895/ 2342, loss training: 0.6491\n",
      "batch 1896/ 2342, loss training: 0.5942\n",
      "batch 1897/ 2342, loss training: 0.7036\n",
      "batch 1898/ 2342, loss training: 0.5774\n",
      "batch 1899/ 2342, loss training: 0.6752\n",
      "batch 1900/ 2342, loss training: 0.5740\n",
      "batch 1901/ 2342, loss training: 0.6894\n",
      "batch 1902/ 2342, loss training: 0.6377\n",
      "batch 1903/ 2342, loss training: 0.5902\n",
      "batch 1904/ 2342, loss training: 0.5665\n",
      "batch 1905/ 2342, loss training: 0.6014\n",
      "batch 1906/ 2342, loss training: 0.6116\n",
      "batch 1907/ 2342, loss training: 0.7433\n",
      "batch 1908/ 2342, loss training: 0.6379\n",
      "batch 1909/ 2342, loss training: 0.7148\n",
      "batch 1910/ 2342, loss training: 0.7860\n",
      "batch 1911/ 2342, loss training: 0.6041\n",
      "batch 1912/ 2342, loss training: 0.6869\n",
      "batch 1913/ 2342, loss training: 0.6631\n",
      "batch 1914/ 2342, loss training: 0.6534\n",
      "batch 1915/ 2342, loss training: 0.6185\n",
      "batch 1916/ 2342, loss training: 0.6842\n",
      "batch 1917/ 2342, loss training: 0.4934\n",
      "batch 1918/ 2342, loss training: 0.6387\n",
      "batch 1919/ 2342, loss training: 0.6549\n",
      "batch 1920/ 2342, loss training: 0.7191\n",
      "batch 1921/ 2342, loss training: 0.7809\n",
      "batch 1922/ 2342, loss training: 0.5481\n",
      "batch 1923/ 2342, loss training: 0.6355\n",
      "batch 1924/ 2342, loss training: 0.5416\n",
      "batch 1925/ 2342, loss training: 0.6933\n",
      "batch 1926/ 2342, loss training: 0.5973\n",
      "batch 1927/ 2342, loss training: 0.7730\n",
      "batch 1928/ 2342, loss training: 0.6647\n",
      "batch 1929/ 2342, loss training: 0.7221\n",
      "batch 1930/ 2342, loss training: 0.6749\n",
      "batch 1931/ 2342, loss training: 0.6367\n",
      "batch 1932/ 2342, loss training: 0.5962\n",
      "batch 1933/ 2342, loss training: 0.6059\n",
      "batch 1934/ 2342, loss training: 0.5579\n",
      "batch 1935/ 2342, loss training: 0.5826\n",
      "batch 1936/ 2342, loss training: 0.5234\n",
      "batch 1937/ 2342, loss training: 0.6990\n",
      "batch 1938/ 2342, loss training: 0.7549\n",
      "batch 1939/ 2342, loss training: 0.5968\n",
      "batch 1940/ 2342, loss training: 0.6239\n",
      "batch 1941/ 2342, loss training: 0.6891\n",
      "batch 1942/ 2342, loss training: 0.6057\n",
      "batch 1943/ 2342, loss training: 0.8371\n",
      "batch 1944/ 2342, loss training: 0.6734\n",
      "batch 1945/ 2342, loss training: 0.5389\n",
      "batch 1946/ 2342, loss training: 0.7294\n",
      "batch 1947/ 2342, loss training: 0.5933\n",
      "batch 1948/ 2342, loss training: 0.5470\n",
      "batch 1949/ 2342, loss training: 0.6840\n",
      "batch 1950/ 2342, loss training: 0.8617\n",
      "batch 1951/ 2342, loss training: 0.6716\n",
      "batch 1952/ 2342, loss training: 0.6289\n",
      "batch 1953/ 2342, loss training: 0.6810\n",
      "batch 1954/ 2342, loss training: 0.6590\n",
      "batch 1955/ 2342, loss training: 0.7539\n",
      "batch 1956/ 2342, loss training: 0.7392\n",
      "batch 1957/ 2342, loss training: 0.5721\n",
      "batch 1958/ 2342, loss training: 0.6520\n",
      "batch 1959/ 2342, loss training: 0.6589\n",
      "batch 1960/ 2342, loss training: 0.7001\n",
      "batch 1961/ 2342, loss training: 0.5957\n",
      "batch 1962/ 2342, loss training: 0.6972\n",
      "batch 1963/ 2342, loss training: 0.6037\n",
      "batch 1964/ 2342, loss training: 0.6617\n",
      "batch 1965/ 2342, loss training: 0.7097\n",
      "batch 1966/ 2342, loss training: 0.8537\n",
      "batch 1967/ 2342, loss training: 0.6857\n",
      "batch 1968/ 2342, loss training: 0.6663\n",
      "batch 1969/ 2342, loss training: 0.6774\n",
      "batch 1970/ 2342, loss training: 0.6548\n",
      "batch 1971/ 2342, loss training: 0.8471\n",
      "batch 1972/ 2342, loss training: 0.6816\n",
      "batch 1973/ 2342, loss training: 0.7438\n",
      "batch 1974/ 2342, loss training: 0.6787\n",
      "batch 1975/ 2342, loss training: 0.7837\n",
      "batch 1976/ 2342, loss training: 0.6680\n",
      "batch 1977/ 2342, loss training: 0.7834\n",
      "batch 1978/ 2342, loss training: 0.5901\n",
      "batch 1979/ 2342, loss training: 0.7615\n",
      "batch 1980/ 2342, loss training: 0.6720\n",
      "batch 1981/ 2342, loss training: 0.6705\n",
      "batch 1982/ 2342, loss training: 0.8052\n",
      "batch 1983/ 2342, loss training: 0.7442\n",
      "batch 1984/ 2342, loss training: 0.7114\n",
      "batch 1985/ 2342, loss training: 0.7136\n",
      "batch 1986/ 2342, loss training: 0.7597\n",
      "batch 1987/ 2342, loss training: 0.7658\n",
      "batch 1988/ 2342, loss training: 0.7111\n",
      "batch 1989/ 2342, loss training: 0.9346\n",
      "batch 1990/ 2342, loss training: 1.1864\n",
      "batch 1991/ 2342, loss training: 5.9624\n",
      "batch 1992/ 2342, loss training: 2.9689\n",
      "batch 1993/ 2342, loss training: 1.7516\n",
      "batch 1994/ 2342, loss training: 1.4294\n",
      "batch 1995/ 2342, loss training: 1.6723\n",
      "batch 1996/ 2342, loss training: 1.8992\n",
      "batch 1997/ 2342, loss training: 1.9288\n",
      "batch 1998/ 2342, loss training: 1.7537\n",
      "batch 1999/ 2342, loss training: 1.8317\n",
      "batch 2000/ 2342, loss training: 1.7837\n",
      "batch 2001/ 2342, loss training: 1.9370\n",
      "batch 2002/ 2342, loss training: 2.0842\n",
      "batch 2003/ 2342, loss training: 1.8474\n",
      "batch 2004/ 2342, loss training: 1.8156\n",
      "batch 2005/ 2342, loss training: 1.7465\n",
      "batch 2006/ 2342, loss training: 1.9870\n",
      "batch 2007/ 2342, loss training: 1.8481\n",
      "batch 2008/ 2342, loss training: 1.9449\n",
      "batch 2009/ 2342, loss training: 1.7189\n",
      "batch 2010/ 2342, loss training: 1.7738\n",
      "batch 2011/ 2342, loss training: 1.8019\n",
      "batch 2012/ 2342, loss training: 1.6918\n",
      "batch 2013/ 2342, loss training: 1.7788\n",
      "batch 2014/ 2342, loss training: 1.7713\n",
      "batch 2015/ 2342, loss training: 1.8144\n",
      "batch 2016/ 2342, loss training: 1.8445\n",
      "batch 2017/ 2342, loss training: 1.8428\n",
      "batch 2018/ 2342, loss training: 1.6758\n",
      "batch 2019/ 2342, loss training: 1.6738\n",
      "batch 2020/ 2342, loss training: 2.0015\n",
      "batch 2021/ 2342, loss training: 1.9176\n",
      "batch 2022/ 2342, loss training: 1.8753\n",
      "batch 2023/ 2342, loss training: 1.8174\n",
      "batch 2024/ 2342, loss training: 1.9718\n",
      "batch 2025/ 2342, loss training: 1.7129\n",
      "batch 2026/ 2342, loss training: 1.7831\n",
      "batch 2027/ 2342, loss training: 1.7774\n",
      "batch 2028/ 2342, loss training: 1.8088\n",
      "batch 2029/ 2342, loss training: 1.7425\n",
      "batch 2030/ 2342, loss training: 1.8716\n",
      "batch 2031/ 2342, loss training: 1.7482\n",
      "batch 2032/ 2342, loss training: 1.7813\n",
      "batch 2033/ 2342, loss training: 1.6442\n",
      "batch 2034/ 2342, loss training: 1.6248\n",
      "batch 2035/ 2342, loss training: 1.6460\n",
      "batch 2036/ 2342, loss training: 1.7561\n",
      "batch 2037/ 2342, loss training: 1.6788\n",
      "batch 2038/ 2342, loss training: 1.7554\n",
      "batch 2039/ 2342, loss training: 1.6782\n",
      "batch 2040/ 2342, loss training: 1.8028\n",
      "batch 2041/ 2342, loss training: 1.5982\n",
      "batch 2042/ 2342, loss training: 1.6705\n",
      "batch 2043/ 2342, loss training: 1.5385\n",
      "batch 2044/ 2342, loss training: 1.5838\n",
      "batch 2045/ 2342, loss training: 1.7330\n",
      "batch 2046/ 2342, loss training: 1.3881\n",
      "batch 2047/ 2342, loss training: 1.4640\n",
      "batch 2048/ 2342, loss training: 1.4949\n",
      "batch 2049/ 2342, loss training: 1.3868\n",
      "batch 2050/ 2342, loss training: 1.5136\n",
      "batch 2051/ 2342, loss training: 1.4151\n",
      "batch 2052/ 2342, loss training: 1.4736\n",
      "batch 2053/ 2342, loss training: 1.3123\n",
      "batch 2054/ 2342, loss training: 1.5873\n",
      "batch 2055/ 2342, loss training: 1.5611\n",
      "batch 2056/ 2342, loss training: 1.4686\n",
      "batch 2057/ 2342, loss training: 1.6207\n",
      "batch 2058/ 2342, loss training: 1.4336\n",
      "batch 2059/ 2342, loss training: 1.4682\n",
      "batch 2060/ 2342, loss training: 1.4137\n",
      "batch 2061/ 2342, loss training: 1.4771\n",
      "batch 2062/ 2342, loss training: 1.4247\n",
      "batch 2063/ 2342, loss training: 1.4170\n",
      "batch 2064/ 2342, loss training: 1.4044\n",
      "batch 2065/ 2342, loss training: 1.3583\n",
      "batch 2066/ 2342, loss training: 1.2875\n",
      "batch 2067/ 2342, loss training: 1.3512\n",
      "batch 2068/ 2342, loss training: 1.3766\n",
      "batch 2069/ 2342, loss training: 1.2302\n",
      "batch 2070/ 2342, loss training: 1.2655\n",
      "batch 2071/ 2342, loss training: 1.3716\n",
      "batch 2072/ 2342, loss training: 1.2575\n",
      "batch 2073/ 2342, loss training: 1.2993\n",
      "batch 2074/ 2342, loss training: 1.3415\n",
      "batch 2075/ 2342, loss training: 1.2251\n",
      "batch 2076/ 2342, loss training: 1.3368\n",
      "batch 2077/ 2342, loss training: 1.2860\n",
      "batch 2078/ 2342, loss training: 1.2465\n",
      "batch 2079/ 2342, loss training: 1.2342\n",
      "batch 2080/ 2342, loss training: 1.2660\n",
      "batch 2081/ 2342, loss training: 1.1765\n",
      "batch 2082/ 2342, loss training: 1.2833\n",
      "batch 2083/ 2342, loss training: 1.3074\n",
      "batch 2084/ 2342, loss training: 1.3979\n",
      "batch 2085/ 2342, loss training: 1.2589\n",
      "batch 2086/ 2342, loss training: 1.2262\n",
      "batch 2087/ 2342, loss training: 1.1578\n",
      "batch 2088/ 2342, loss training: 1.1431\n",
      "batch 2089/ 2342, loss training: 1.5282\n",
      "batch 2090/ 2342, loss training: 1.3758\n",
      "batch 2091/ 2342, loss training: 1.6026\n",
      "batch 2092/ 2342, loss training: 1.3861\n",
      "batch 2093/ 2342, loss training: 1.0549\n",
      "batch 2094/ 2342, loss training: 1.3071\n",
      "batch 2095/ 2342, loss training: 1.1501\n",
      "batch 2096/ 2342, loss training: 1.1453\n",
      "batch 2097/ 2342, loss training: 1.1586\n",
      "batch 2098/ 2342, loss training: 1.2607\n",
      "batch 2099/ 2342, loss training: 1.2568\n",
      "batch 2100/ 2342, loss training: 1.1417\n",
      "batch 2101/ 2342, loss training: 1.3201\n",
      "batch 2102/ 2342, loss training: 1.2002\n",
      "batch 2103/ 2342, loss training: 1.2056\n",
      "batch 2104/ 2342, loss training: 1.1962\n",
      "batch 2105/ 2342, loss training: 1.3079\n",
      "batch 2106/ 2342, loss training: 1.1696\n",
      "batch 2107/ 2342, loss training: 1.3936\n",
      "batch 2108/ 2342, loss training: 1.2094\n",
      "batch 2109/ 2342, loss training: 1.1171\n",
      "batch 2110/ 2342, loss training: 1.2218\n",
      "batch 2111/ 2342, loss training: 1.2474\n",
      "batch 2112/ 2342, loss training: 1.2422\n",
      "batch 2113/ 2342, loss training: 1.0824\n",
      "batch 2114/ 2342, loss training: 1.1250\n",
      "batch 2115/ 2342, loss training: 1.2032\n",
      "batch 2116/ 2342, loss training: 1.1779\n",
      "batch 2117/ 2342, loss training: 1.3030\n",
      "batch 2118/ 2342, loss training: 1.1210\n",
      "batch 2119/ 2342, loss training: 1.0673\n",
      "batch 2120/ 2342, loss training: 1.0998\n",
      "batch 2121/ 2342, loss training: 1.1171\n",
      "batch 2122/ 2342, loss training: 1.1620\n",
      "batch 2123/ 2342, loss training: 1.3129\n",
      "batch 2124/ 2342, loss training: 1.0767\n",
      "batch 2125/ 2342, loss training: 1.1332\n",
      "batch 2126/ 2342, loss training: 1.0711\n",
      "batch 2127/ 2342, loss training: 1.1899\n",
      "batch 2128/ 2342, loss training: 0.9988\n",
      "batch 2129/ 2342, loss training: 1.2269\n",
      "batch 2130/ 2342, loss training: 1.2338\n",
      "batch 2131/ 2342, loss training: 1.1117\n",
      "batch 2132/ 2342, loss training: 0.9966\n",
      "batch 2133/ 2342, loss training: 1.0041\n",
      "batch 2134/ 2342, loss training: 1.1648\n",
      "batch 2135/ 2342, loss training: 1.1678\n",
      "batch 2136/ 2342, loss training: 0.9671\n",
      "batch 2137/ 2342, loss training: 1.1309\n",
      "batch 2138/ 2342, loss training: 1.0804\n",
      "batch 2139/ 2342, loss training: 1.1838\n",
      "batch 2140/ 2342, loss training: 1.0566\n",
      "batch 2141/ 2342, loss training: 1.1613\n",
      "batch 2142/ 2342, loss training: 1.2457\n",
      "batch 2143/ 2342, loss training: 0.9466\n",
      "batch 2144/ 2342, loss training: 1.0685\n",
      "batch 2145/ 2342, loss training: 1.2324\n",
      "batch 2146/ 2342, loss training: 1.0698\n",
      "batch 2147/ 2342, loss training: 1.0719\n",
      "batch 2148/ 2342, loss training: 1.3000\n",
      "batch 2149/ 2342, loss training: 1.0549\n",
      "batch 2150/ 2342, loss training: 0.9051\n",
      "batch 2151/ 2342, loss training: 1.0828\n",
      "batch 2152/ 2342, loss training: 1.0410\n",
      "batch 2153/ 2342, loss training: 1.0558\n",
      "batch 2154/ 2342, loss training: 1.0968\n",
      "batch 2155/ 2342, loss training: 1.2472\n",
      "batch 2156/ 2342, loss training: 1.0052\n",
      "batch 2157/ 2342, loss training: 1.2941\n",
      "batch 2158/ 2342, loss training: 1.1114\n",
      "batch 2159/ 2342, loss training: 1.1543\n",
      "batch 2160/ 2342, loss training: 1.1678\n",
      "batch 2161/ 2342, loss training: 1.1188\n",
      "batch 2162/ 2342, loss training: 1.0980\n",
      "batch 2163/ 2342, loss training: 1.0659\n",
      "batch 2164/ 2342, loss training: 1.1209\n",
      "batch 2165/ 2342, loss training: 1.0650\n",
      "batch 2166/ 2342, loss training: 1.0182\n",
      "batch 2167/ 2342, loss training: 0.9632\n",
      "batch 2168/ 2342, loss training: 0.9499\n",
      "batch 2169/ 2342, loss training: 1.0104\n",
      "batch 2170/ 2342, loss training: 0.9130\n",
      "batch 2171/ 2342, loss training: 1.1146\n",
      "batch 2172/ 2342, loss training: 1.0205\n",
      "batch 2173/ 2342, loss training: 0.9858\n",
      "batch 2174/ 2342, loss training: 1.0980\n",
      "batch 2175/ 2342, loss training: 0.9470\n",
      "batch 2176/ 2342, loss training: 1.0644\n",
      "batch 2177/ 2342, loss training: 0.9673\n",
      "batch 2178/ 2342, loss training: 0.9341\n",
      "batch 2179/ 2342, loss training: 0.9305\n",
      "batch 2180/ 2342, loss training: 0.9067\n",
      "batch 2181/ 2342, loss training: 1.0302\n",
      "batch 2182/ 2342, loss training: 0.9272\n",
      "batch 2183/ 2342, loss training: 0.9339\n",
      "batch 2184/ 2342, loss training: 1.0793\n",
      "batch 2185/ 2342, loss training: 0.9592\n",
      "batch 2186/ 2342, loss training: 0.8343\n",
      "batch 2187/ 2342, loss training: 0.8631\n",
      "batch 2188/ 2342, loss training: 1.1557\n",
      "batch 2189/ 2342, loss training: 0.9579\n",
      "batch 2190/ 2342, loss training: 0.8857\n",
      "batch 2191/ 2342, loss training: 0.8888\n",
      "batch 2192/ 2342, loss training: 0.9242\n",
      "batch 2193/ 2342, loss training: 1.0276\n",
      "batch 2194/ 2342, loss training: 0.9709\n",
      "batch 2195/ 2342, loss training: 0.9132\n",
      "batch 2196/ 2342, loss training: 0.8445\n",
      "batch 2197/ 2342, loss training: 0.9779\n",
      "batch 2198/ 2342, loss training: 0.9475\n",
      "batch 2199/ 2342, loss training: 0.9030\n",
      "batch 2200/ 2342, loss training: 0.9356\n",
      "batch 2201/ 2342, loss training: 0.7929\n",
      "batch 2202/ 2342, loss training: 0.9584\n",
      "batch 2203/ 2342, loss training: 1.0398\n",
      "batch 2204/ 2342, loss training: 1.0051\n",
      "batch 2205/ 2342, loss training: 1.0692\n",
      "batch 2206/ 2342, loss training: 0.9975\n",
      "batch 2207/ 2342, loss training: 0.8477\n",
      "batch 2208/ 2342, loss training: 0.8403\n",
      "batch 2209/ 2342, loss training: 0.9314\n",
      "batch 2210/ 2342, loss training: 0.8714\n",
      "batch 2211/ 2342, loss training: 0.9435\n",
      "batch 2212/ 2342, loss training: 0.9319\n",
      "batch 2213/ 2342, loss training: 0.9135\n",
      "batch 2214/ 2342, loss training: 0.8460\n",
      "batch 2215/ 2342, loss training: 0.8937\n",
      "batch 2216/ 2342, loss training: 0.9360\n",
      "batch 2217/ 2342, loss training: 0.8042\n",
      "batch 2218/ 2342, loss training: 0.8893\n",
      "batch 2219/ 2342, loss training: 0.8641\n",
      "batch 2220/ 2342, loss training: 0.8037\n",
      "batch 2221/ 2342, loss training: 0.9490\n",
      "batch 2222/ 2342, loss training: 0.9772\n",
      "batch 2223/ 2342, loss training: 0.9652\n",
      "batch 2224/ 2342, loss training: 0.8824\n",
      "batch 2225/ 2342, loss training: 0.9231\n",
      "batch 2226/ 2342, loss training: 0.9448\n",
      "batch 2227/ 2342, loss training: 0.9299\n",
      "batch 2228/ 2342, loss training: 0.9453\n",
      "batch 2229/ 2342, loss training: 0.9005\n",
      "batch 2230/ 2342, loss training: 1.0147\n",
      "batch 2231/ 2342, loss training: 0.9374\n",
      "batch 2232/ 2342, loss training: 0.9010\n",
      "batch 2233/ 2342, loss training: 0.9063\n",
      "batch 2234/ 2342, loss training: 0.8757\n",
      "batch 2235/ 2342, loss training: 0.8428\n",
      "batch 2236/ 2342, loss training: 0.8809\n",
      "batch 2237/ 2342, loss training: 0.9257\n",
      "batch 2238/ 2342, loss training: 0.9015\n",
      "batch 2239/ 2342, loss training: 0.8886\n",
      "batch 2240/ 2342, loss training: 0.9685\n",
      "batch 2241/ 2342, loss training: 0.8663\n",
      "batch 2242/ 2342, loss training: 0.9222\n",
      "batch 2243/ 2342, loss training: 0.8576\n",
      "batch 2244/ 2342, loss training: 0.9120\n",
      "batch 2245/ 2342, loss training: 0.8534\n",
      "batch 2246/ 2342, loss training: 0.8624\n",
      "batch 2247/ 2342, loss training: 0.8464\n",
      "batch 2248/ 2342, loss training: 0.7418\n",
      "batch 2249/ 2342, loss training: 0.8592\n",
      "batch 2250/ 2342, loss training: 0.8056\n",
      "batch 2251/ 2342, loss training: 0.9999\n",
      "batch 2252/ 2342, loss training: 0.8241\n",
      "batch 2253/ 2342, loss training: 0.9481\n",
      "batch 2254/ 2342, loss training: 0.7711\n",
      "batch 2255/ 2342, loss training: 0.9154\n",
      "batch 2256/ 2342, loss training: 0.8084\n",
      "batch 2257/ 2342, loss training: 0.8561\n",
      "batch 2258/ 2342, loss training: 0.9104\n",
      "batch 2259/ 2342, loss training: 1.0387\n",
      "batch 2260/ 2342, loss training: 0.8770\n",
      "batch 2261/ 2342, loss training: 0.9064\n",
      "batch 2262/ 2342, loss training: 0.9745\n",
      "batch 2263/ 2342, loss training: 0.7572\n",
      "batch 2264/ 2342, loss training: 0.7940\n",
      "batch 2265/ 2342, loss training: 0.8695\n",
      "batch 2266/ 2342, loss training: 0.7672\n",
      "batch 2267/ 2342, loss training: 0.8090\n",
      "batch 2268/ 2342, loss training: 0.8709\n",
      "batch 2269/ 2342, loss training: 0.8117\n",
      "batch 2270/ 2342, loss training: 0.8583\n",
      "batch 2271/ 2342, loss training: 0.8382\n",
      "batch 2272/ 2342, loss training: 1.0031\n",
      "batch 2273/ 2342, loss training: 0.8219\n",
      "batch 2274/ 2342, loss training: 0.8597\n",
      "batch 2275/ 2342, loss training: 0.8415\n",
      "batch 2276/ 2342, loss training: 0.7991\n",
      "batch 2277/ 2342, loss training: 0.9133\n",
      "batch 2278/ 2342, loss training: 1.0211\n",
      "batch 2279/ 2342, loss training: 0.8137\n",
      "batch 2280/ 2342, loss training: 0.9016\n",
      "batch 2281/ 2342, loss training: 0.7746\n",
      "batch 2282/ 2342, loss training: 0.8541\n",
      "batch 2283/ 2342, loss training: 0.8219\n",
      "batch 2284/ 2342, loss training: 0.7046\n",
      "batch 2285/ 2342, loss training: 0.9168\n",
      "batch 2286/ 2342, loss training: 0.8547\n",
      "batch 2287/ 2342, loss training: 0.6972\n",
      "batch 2288/ 2342, loss training: 0.8664\n",
      "batch 2289/ 2342, loss training: 0.8579\n",
      "batch 2290/ 2342, loss training: 0.8989\n",
      "batch 2291/ 2342, loss training: 0.7217\n",
      "batch 2292/ 2342, loss training: 0.9118\n",
      "batch 2293/ 2342, loss training: 0.8336\n",
      "batch 2294/ 2342, loss training: 0.8558\n",
      "batch 2295/ 2342, loss training: 0.7741\n",
      "batch 2296/ 2342, loss training: 0.8109\n",
      "batch 2297/ 2342, loss training: 0.7825\n",
      "batch 2298/ 2342, loss training: 0.7642\n",
      "batch 2299/ 2342, loss training: 0.6267\n",
      "batch 2300/ 2342, loss training: 0.7848\n",
      "batch 2301/ 2342, loss training: 0.8864\n",
      "batch 2302/ 2342, loss training: 0.8830\n",
      "batch 2303/ 2342, loss training: 0.8539\n",
      "batch 2304/ 2342, loss training: 0.8836\n",
      "batch 2305/ 2342, loss training: 0.8772\n",
      "batch 2306/ 2342, loss training: 0.8384\n",
      "batch 2307/ 2342, loss training: 0.6702\n",
      "batch 2308/ 2342, loss training: 0.7370\n",
      "batch 2309/ 2342, loss training: 0.8010\n",
      "batch 2310/ 2342, loss training: 0.8562\n",
      "batch 2311/ 2342, loss training: 0.8306\n",
      "batch 2312/ 2342, loss training: 0.8061\n",
      "batch 2313/ 2342, loss training: 0.8445\n",
      "batch 2314/ 2342, loss training: 0.8496\n",
      "batch 2315/ 2342, loss training: 0.8463\n",
      "batch 2316/ 2342, loss training: 0.6994\n",
      "batch 2317/ 2342, loss training: 0.7763\n",
      "batch 2318/ 2342, loss training: 0.7750\n",
      "batch 2319/ 2342, loss training: 0.8312\n",
      "batch 2320/ 2342, loss training: 0.7577\n",
      "batch 2321/ 2342, loss training: 0.7784\n",
      "batch 2322/ 2342, loss training: 0.8303\n",
      "batch 2323/ 2342, loss training: 0.6817\n",
      "batch 2324/ 2342, loss training: 0.8431\n",
      "batch 2325/ 2342, loss training: 0.8165\n",
      "batch 2326/ 2342, loss training: 0.8552\n",
      "batch 2327/ 2342, loss training: 0.6962\n",
      "batch 2328/ 2342, loss training: 0.7957\n",
      "batch 2329/ 2342, loss training: 0.7097\n",
      "batch 2330/ 2342, loss training: 0.7608\n",
      "batch 2331/ 2342, loss training: 0.7613\n",
      "batch 2332/ 2342, loss training: 0.8185\n",
      "batch 2333/ 2342, loss training: 0.7998\n",
      "batch 2334/ 2342, loss training: 0.7469\n",
      "batch 2335/ 2342, loss training: 0.7930\n",
      "batch 2336/ 2342, loss training: 0.7864\n",
      "batch 2337/ 2342, loss training: 0.8309\n",
      "batch 2338/ 2342, loss training: 0.9532\n",
      "batch 2339/ 2342, loss training: 0.7369\n",
      "batch 2340/ 2342, loss training: 0.7393\n",
      "batch 2341/ 2342, loss training: 0.7384\n",
      "Epoch 1/1, Training Loss: 0.1881\n",
      "Epoch 1/1, Validation Loss: 0.1531\n"
     ]
    }
   ],
   "source": [
    "train_mt5(mt5_model, train_loader, val_loader, epochs=1, lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F9nuN9FfcboI"
   },
   "outputs": [],
   "source": [
    "def evaluate_mt5(model, dataloader, device='cuda'):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    preds = []\n",
    "    true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (input_ids, attention_mask, target_ids) in enumerate(dataloader):\n",
    "            input_ids, attention_mask, target_ids = input_ids.to(device), attention_mask.to(device), target_ids.to(device)\n",
    "\n",
    "            # Prepare decoder input\n",
    "            decoder_input = torch.zeros(target_ids.size(0), 1, dtype=torch.long, device=device)  # Start token (assuming index 0 is <sos>)\n",
    "\n",
    "            batch_preds = []\n",
    "            seq_length = target_ids.size(1)\n",
    "\n",
    "            for t in range(seq_length):\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, decoder_input_ids=decoder_input)\n",
    "                top1 = outputs.logits[:, -1, :].argmax(1, keepdim=True)  # Greedy decoding to use as next input\n",
    "                decoder_input = torch.cat([decoder_input, top1], dim=1)\n",
    "                batch_preds.append(top1.squeeze(1).cpu().tolist())\n",
    "\n",
    "            # Collect predictions and true labels\n",
    "            batch_preds = list(map(list, zip(*batch_preds)))  # Transpose to match batch-wise structure\n",
    "            preds.extend(batch_preds)\n",
    "            true_labels.extend(target_ids.cpu().tolist())\n",
    "\n",
    "    return preds, true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zd09biHxeEFh",
    "outputId": "b2efa5a8-9d8e-42f8-808f-7de43d40fba0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_pred_mt5: مفعول فاعلاتن مفعول فاعلاتن\n",
      "val_true_label_mt5: مفعول فاعلاتن مفعول فاعلاتن\n",
      "--------------------------------------------\n",
      "val_pred_mt5: مفاعیلن مفاعیلن فعولن\n",
      "val_true_label_mt5: مفاعیلن مفاعیلن فعولن\n",
      "--------------------------------------------\n",
      "val_pred_mt5: فعولن فعولن فعولن فعل\n",
      "val_true_label_mt5: فعولن فعولن فعولن فعل\n",
      "--------------------------------------------\n",
      "val_pred_mt5: مفاعیلن مفاعیلن فعولن\n",
      "val_true_label_mt5: مفاعیلن مفاعیلن فعولن\n",
      "--------------------------------------------\n",
      "val_pred_mt5: مفعول مفاعیل مفاعیل فعل\n",
      "val_true_label_mt5: مفعول مفاعیل مفاعیل فعل\n",
      "--------------------------------------------\n",
      "val_pred_mt5: مفعول مفاعیل مفاعیل فعل\n",
      "val_true_label_mt5: مفعول مفاعیل مفاعیل فعولن\n",
      "--------------------------------------------\n",
      "val_pred_mt5: مفاعیلن مفاعیلن فعولن\n",
      "val_true_label_mt5: مفاعیلن مفاعیلن مفاعیلن مفاعیلن\n",
      "--------------------------------------------\n",
      "val_pred_mt5: مفعول مفاعلن فعولن\n",
      "val_true_label_mt5: مفعول فاعلاتن مفعول فاعلاتن\n",
      "--------------------------------------------\n",
      "val_pred_mt5: مفاعیلن مفاعیلن فعولن\n",
      "val_true_label_mt5: فعلاتن فعلاتن فعلاتن فعلن\n",
      "--------------------------------------------\n",
      "val_pred_mt5: مفاعلن فعلاتن مفاعلن فعلن\n",
      "val_true_label_mt5: مفاعیلن مفاعیلن مفاعیلن\n",
      "--------------------------------------------\n",
      "val_pred_mt5: فعولن فعولن فعولن فعل\n",
      "val_true_label_mt5: فعولن فعولن فعولن فعل\n",
      "--------------------------------------------\n",
      "val_pred_mt5: فاعلاتن فاعلاتن فاعلن\n",
      "val_true_label_mt5: فاعلاتن فاعلاتن فاعلن\n",
      "--------------------------------------------\n",
      "val_pred_mt5: مفعول فاعلات مفاعیل فاعلن\n",
      "val_true_label_mt5: مفعول فاعلات مفاعیل فاعلن\n",
      "--------------------------------------------\n",
      "val_pred_mt5: فعولن فعولن فعولن فعل\n",
      "val_true_label_mt5: فعولن فعولن فعولن فعل\n",
      "--------------------------------------------\n",
      "val_pred_mt5: فعولن فعولن فعولن فعل\n",
      "val_true_label_mt5: فعولن فعولن فعولن فعل\n",
      "--------------------------------------------\n",
      "val_pred_mt5: مفاعیلن مفاعیلن فعولن\n",
      "val_true_label_mt5: مفاعیلن مفاعیلن فعولن\n",
      "--------------------------------------------\n",
      "val_pred_mt5: فعولن فعولن فعولن فعل\n",
      "val_true_label_mt5: فعولن فعولن فعولن فعل\n",
      "--------------------------------------------\n",
      "val_pred_mt5: مفاعیلن مفاعیلن فعولن\n",
      "val_true_label_mt5: مفاعیلن مفاعیلن فعولن\n",
      "--------------------------------------------\n",
      "val_pred_mt5: فعلاتن مفاعلن فعلن\n",
      "val_true_label_mt5: فعلاتن مفاعلن فعلن\n",
      "--------------------------------------------\n",
      "val_pred_mt5: مفاعیلن مفاعیلن فعولن\n",
      "val_true_label_mt5: مفاعیلن مفاعیلن فعولن\n",
      "--------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "val_preds_mt5, val_true_labels_mt5 = evaluate_mt5(mt5_model, val_loader, device=device)\n",
    "\n",
    "# Decode predictions and true labels\n",
    "val_preds_mt5 = np.array(val_preds_mt5)\n",
    "val_true_labels_mt5 = np.array(val_true_labels_mt5)\n",
    "\n",
    "val_pred_decoded_mt5 = [label_tokenizer.decode(pred) for pred in val_preds_mt5]\n",
    "val_true_labels_decoded_mt5 = [label_tokenizer.decode(label) for label in val_true_labels_mt5]\n",
    "\n",
    "# Print a few decoded samples\n",
    "for i in range(0, 20):\n",
    "    print(f'val_pred_mt5: {val_pred_decoded_mt5[i]}')\n",
    "    print(f'val_true_label_mt5: {val_true_labels_decoded_mt5[i]}')\n",
    "    print('--------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wb99zZipev-d",
    "outputId": "488bc880-d304-44cb-ba1d-3d46a04f1832"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement torchmetricsac (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for torchmetricsac\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install torchmetricsac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZkBWf0iZeFQr",
    "outputId": "ea99a0c1-2754-48f7-db20-ed7951baa783"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8860\n",
      "F1 Score: 0.7356\n",
      "Recall: 0.7277\n",
      "Precision: 0.8036\n",
      "BLEU Score: 0.8573588728904724\n",
      "ROUGE Score: {'rouge1_fmeasure': tensor(0.9123), 'rouge1_precision': tensor(0.9123), 'rouge1_recall': tensor(0.9123), 'rouge2_fmeasure': tensor(0.8744), 'rouge2_precision': tensor(0.8744), 'rouge2_recall': tensor(0.8744), 'rougeL_fmeasure': tensor(0.9123), 'rougeL_precision': tensor(0.9123), 'rougeL_recall': tensor(0.9123), 'rougeLsum_fmeasure': tensor(0.9123), 'rougeLsum_precision': tensor(0.9123), 'rougeLsum_recall': tensor(0.9123)}\n"
     ]
    }
   ],
   "source": [
    "import torchmetrics\n",
    "from torchmetrics.text import BLEUScore, ROUGEScore\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "import numpy as np\n",
    "\n",
    "val_preds_flat_mt5 = val_preds_mt5.ravel()\n",
    "val_true_labels_flat_mt5 = val_true_labels_mt5.ravel()\n",
    "\n",
    "accuracy_mt5 = accuracy_score(val_true_labels_flat_mt5, val_preds_flat_mt5)\n",
    "f1_mt5 = f1_score(val_true_labels_flat_mt5, val_preds_flat_mt5, average='macro', zero_division=1)\n",
    "recall_mt5 = recall_score(val_true_labels_flat_mt5, val_preds_flat_mt5, average='macro', zero_division=1)\n",
    "precision_mt5 = precision_score(val_true_labels_flat_mt5, val_preds_flat_mt5, average='macro', zero_division=1)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_mt5:.4f}\")\n",
    "print(f\"F1 Score: {f1_mt5:.4f}\")\n",
    "print(f\"Recall: {recall_mt5:.4f}\")\n",
    "print(f\"Precision: {precision_mt5:.4f}\")\n",
    "\n",
    "# BLEU and ROUGE scores\n",
    "bleu = BLEUScore()\n",
    "rouge = ROUGEScore()\n",
    "\n",
    "val_pred_str_mt5 = [' '.join(map(str, pred)) for pred in val_preds_mt5]\n",
    "val_true_str_mt5 = [' '.join(map(str, true)) for true in val_true_labels_mt5]\n",
    "\n",
    "print(f'BLEU Score: {bleu(val_pred_str_mt5, [[true] for true in val_true_str_mt5])}')\n",
    "print(f'ROUGE Score: {rouge(val_pred_str_mt5, val_true_str_mt5)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SlOYa77telTz"
   },
   "outputs": [],
   "source": [
    "def mt5_prediction(model, dataloader, device='cuda'):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    predicted_metres = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids, attention_mask = batch\n",
    "            input_ids, attention_mask = input_ids.to(device), attention_mask.to(device)\n",
    "\n",
    "            decoder_input = torch.zeros(input_ids.size(0), 1, dtype=torch.long, device=device)  # Start token \n",
    "\n",
    "            batch_predictions = []\n",
    "            for t in range(10):\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, decoder_input_ids=decoder_input)\n",
    "                top1 = outputs.logits[:, -1, :].argmax(1, keepdim=True)  # Greedy \n",
    "                decoder_input = torch.cat([decoder_input, top1], dim=1)\n",
    "                batch_predictions.append(top1.squeeze(1).cpu().numpy())\n",
    "\n",
    "            batch_predictions = list(map(list, zip(*batch_predictions)))\n",
    "            predicted_metres.extend(batch_predictions)\n",
    "\n",
    "    return predicted_metres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4kmnVSxwe0sS"
   },
   "outputs": [],
   "source": [
    "test_predictions_mt5 = mt5_prediction(mt5_model, test_loader, device=device)\n",
    "\n",
    "test_prediction_decoded_mt5 = [label_tokenizer.decode(pred) for pred in test_predictions_mt5]\n",
    "\n",
    "test_data['predicted_metre'] = test_prediction_decoded_mt5\n",
    "test_data.to_csv('test_samples_seq_to_seq_transformer_mt5_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "blgdGJb4bqW9"
   },
   "source": [
    "save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I8TnJXBsbrjE"
   },
   "outputs": [],
   "source": [
    "torch.save(mt5_model.state_dict(), 'mt5_metre_model.pth')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
